{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5794f5ee",
   "metadata": {},
   "source": [
    "# I. Triton Inference Server Setups and Basic Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16ce49",
   "metadata": {},
   "source": [
    "## 0. Assumptions - Following are already installed and available"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95cee65f",
   "metadata": {},
   "source": [
    "1. docker\n",
    "2. python-3\n",
    "3. Miniconda\n",
    "4. For GPUs\n",
    "    4.1. NVIDIA Driver - To run nvidia-smi\n",
    "    4.2. NVIDIA Container Toolkit - Has runtime library to leverage NVIDIA GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8abc2",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932eefec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 25 21:19:22 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0    52W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !docker\n",
    "# !python --version\n",
    "# !conda --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e472c52",
   "metadata": {},
   "source": [
    "## 1. Triton Server Installation (Docker)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf22a177",
   "metadata": {},
   "source": [
    "$ docker pull nvcr.io/nvidia/tritonserver:<yy.mm>-py3\n",
    "# <yy.mm> => 23.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debb7b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.01-py3: Pulling from nvidia/tritonserver\r\n",
      "Digest: sha256:a9133b4f34aefaa2aebdb2009ae134cd220892cf16d3ed45e3e01362b094d732\r\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:23.01-py3\r\n",
      "nvcr.io/nvidia/tritonserver:23.01-py3\r\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "!docker pull nvcr.io/nvidia/tritonserver:23.01-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11fa374",
   "metadata": {},
   "source": [
    "## 2. Code repository structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5a6669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./model_repository_single_service\u001b[00m\r\n",
      "└── \u001b[01;34msentiment-nltk-service\u001b[00m\r\n",
      "    ├── \u001b[01;34m1\u001b[00m\r\n",
      "    │   ├── __init__.py\r\n",
      "    │   ├── model.py\r\n",
      "    │   └── \u001b[01;34mresources\u001b[00m\r\n",
      "    │       └── \u001b[01;34mnltk\u001b[00m\r\n",
      "    │           └── \u001b[01;34msentiment\u001b[00m\r\n",
      "    │               └── \u001b[01;31mvader_lexicon.zip\u001b[00m\r\n",
      "    ├── \u001b[01;32mbuild_env.sh\u001b[00m\r\n",
      "    ├── config.pbtxt\r\n",
      "    ├── requirements.txt\r\n",
      "    └── \u001b[01;31msentinltkenv.tar.gz\u001b[00m\r\n",
      "\r\n",
      "5 directories, 7 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ./model_repository_single_service -I '__pycache__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02229b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"sentiment-nltk-service\"\r\n",
      "backend: \"python\"  # PyTorch, TF, ONNX, TensorRT\r\n",
      "max_batch_size: 8\r\n",
      "\r\n",
      "dynamic_batching { }\r\n",
      "\r\n",
      "input [\r\n",
      "  {\r\n",
      "    name: \"TEXT\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims:  [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "output [\r\n",
      "  {\r\n",
      "    name: \"STATUS\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims: [1]\r\n",
      "  },\r\n",
      "  {\r\n",
      "    name: \"SCORE\"\r\n",
      "    data_type: TYPE_FP32\r\n",
      "    dims: [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "parameters: {\r\n",
      "  key: \"EXECUTION_ENV_PATH\",\r\n",
      "  value: {string_value: \"/mnt/data/model_repository/sentiment-nltk-service/sentinltkenv.tar.gz\"}\r\n",
      "}\r\n",
      "\r\n",
      "instance_group [\r\n",
      "  {\r\n",
      "    count: 1\r\n",
      "    kind: KIND_CPU\r\n",
      "  }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!cat model_repository_single_service/sentiment-nltk-service/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62585c81",
   "metadata": {},
   "source": [
    "## 3. Run Inference Server using docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62dfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_id=!(docker run -d \\\n",
    "                --shm-size=5G \\\n",
    "                -p8000:8000 -p8001:8001 -p8002:8002 \\\n",
    "                -v $PWD/model_repository_single_service:/mnt/data/model_repository \\\n",
    "                nvcr.io/nvidia/tritonserver:23.01-py3 \\\n",
    "                tritonserver \\\n",
    "                --model-repository=/mnt/data/model_repository \\\n",
    "                --log-verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a0f3b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6099717b6c56a70498a528171a34faf4b9cc776dc14851d36b3da19ee0cf876b']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc70add9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "=============================\r\n",
      "== Triton Inference Server ==\r\n",
      "=============================\r\n",
      "\r\n",
      "NVIDIA Release 23.01 (build 52277748)\r\n",
      "Triton Server Version 2.30.0\r\n",
      "\r\n",
      "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\r\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n",
      "\r\n",
      "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n",
      "   Use the NVIDIA Container Toolkit to start this container with GPU support; see\r\n",
      "   https://docs.nvidia.com/datacenter/cloud-native/ .\r\n",
      "\r\n",
      "W0226 05:32:47.599411 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\n",
      "I0226 05:32:47.650953 1 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\n",
      "I0226 05:32:47.662495 1 model_config_utils.cc:646] Server side auto-completed config: name: \"sentiment-nltk-service\"\r\n",
      "max_batch_size: 8\r\n",
      "input {\r\n",
      "  name: \"TEXT\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "output {\r\n",
      "  name: \"STATUS\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "output {\r\n",
      "  name: \"SCORE\"\r\n",
      "  data_type: TYPE_FP32\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "instance_group {\r\n",
      "  count: 1\r\n",
      "  kind: KIND_CPU\r\n",
      "}\r\n",
      "default_model_filename: \"model.py\"\r\n",
      "dynamic_batching {\r\n",
      "}\r\n",
      "parameters {\r\n",
      "  key: \"EXECUTION_ENV_PATH\"\r\n",
      "  value {\r\n",
      "    string_value: \"/mnt/data/model_repository/sentiment-nltk-service/sentinltkenv.tar.gz\"\r\n",
      "  }\r\n",
      "}\r\n",
      "backend: \"python\"\r\n",
      "\r\n",
      "W0226 05:32:47.668431 1 model_lifecycle.cc:107] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\r\n",
      "I0226 05:32:47.668475 1 model_lifecycle.cc:459] loading: sentiment-nltk-service:1\r\n",
      "I0226 05:32:47.668705 1 backend_model.cc:348] Adding default backend config setting: default-max-batch-size,4\r\n",
      "I0226 05:32:47.668746 1 shared_library.cc:112] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\r\n",
      "I0226 05:32:47.919473 1 python_be.cc:1614] 'python' TRITONBACKEND API version: 1.11\r\n",
      "I0226 05:32:47.919504 1 python_be.cc:1636] backend configuration:\r\n",
      "{\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\r\n",
      "I0226 05:32:47.921451 1 python_be.cc:1766] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\r\n",
      "I0226 05:32:47.921755 1 python_be.cc:2012] TRITONBACKEND_GetBackendAttribute: setting attributes\r\n",
      "I0226 05:32:47.921801 1 python_be.cc:1814] TRITONBACKEND_ModelInitialize: sentiment-nltk-service (version 1)\r\n",
      "I0226 05:32:47.923867 1 model_config_utils.cc:1838] ModelConfig 64-bit fields:\r\n",
      "I0226 05:32:47.923891 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\r\n",
      "I0226 05:32:47.923897 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\r\n",
      "I0226 05:32:47.923901 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\r\n",
      "I0226 05:32:47.923906 1 model_config_utils.cc:1840] \tModelConfig::ensemble_scheduling::step::model_version\r\n",
      "I0226 05:32:47.923911 1 model_config_utils.cc:1840] \tModelConfig::input::dims\r\n",
      "I0226 05:32:47.923916 1 model_config_utils.cc:1840] \tModelConfig::input::reshape::shape\r\n",
      "I0226 05:32:47.923920 1 model_config_utils.cc:1840] \tModelConfig::instance_group::secondary_devices::device_id\r\n",
      "I0226 05:32:47.923925 1 model_config_utils.cc:1840] \tModelConfig::model_warmup::inputs::value::dims\r\n",
      "I0226 05:32:47.923930 1 model_config_utils.cc:1840] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\r\n",
      "I0226 05:32:47.923935 1 model_config_utils.cc:1840] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\r\n",
      "I0226 05:32:47.923940 1 model_config_utils.cc:1840] \tModelConfig::output::dims\r\n",
      "I0226 05:32:47.923945 1 model_config_utils.cc:1840] \tModelConfig::output::reshape::shape\r\n",
      "I0226 05:32:47.923950 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\r\n",
      "I0226 05:32:47.923955 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\r\n",
      "I0226 05:32:47.923960 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\r\n",
      "I0226 05:32:47.923965 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::state::dims\r\n",
      "I0226 05:32:47.923970 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::state::initial_state::dims\r\n",
      "I0226 05:32:47.923975 1 model_config_utils.cc:1840] \tModelConfig::version_policy::specific::versions\r\n",
      "I0226 05:32:47.968273 1 python_be.cc:1505] Using Python execution env /mnt/data/model_repository/sentiment-nltk-service/sentinltkenv.tar.gz\r\n",
      "I0226 05:32:50.205134 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_IhVPpg/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_IhVPpg/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-nltk-service/1/model.py triton_python_backend_shm_region_1 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-nltk-service\r\n",
      "I0226 05:32:52.011420 1 python_be.cc:1594] model configuration:\r\n",
      "{\r\n",
      "    \"name\": \"sentiment-nltk-service\",\r\n",
      "    \"platform\": \"\",\r\n",
      "    \"backend\": \"python\",\r\n",
      "    \"version_policy\": {\r\n",
      "        \"latest\": {\r\n",
      "            \"num_versions\": 1\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"max_batch_size\": 8,\r\n",
      "    \"input\": [\r\n",
      "        {\r\n",
      "            \"name\": \"TEXT\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"format\": \"FORMAT_NONE\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"is_shape_tensor\": false,\r\n",
      "            \"allow_ragged_batch\": false,\r\n",
      "            \"optional\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"output\": [\r\n",
      "        {\r\n",
      "            \"name\": \"STATUS\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"SCORE\",\r\n",
      "            \"data_type\": \"TYPE_FP32\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"batch_input\": [],\r\n",
      "    \"batch_output\": [],\r\n",
      "    \"optimization\": {\r\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\r\n",
      "        \"input_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"output_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"gather_kernel_buffer_threshold\": 0,\r\n",
      "        \"eager_batching\": false\r\n",
      "    },\r\n",
      "    \"dynamic_batching\": {\r\n",
      "        \"preferred_batch_size\": [\r\n",
      "            8\r\n",
      "        ],\r\n",
      "        \"max_queue_delay_microseconds\": 0,\r\n",
      "        \"preserve_ordering\": false,\r\n",
      "        \"priority_levels\": 0,\r\n",
      "        \"default_priority_level\": 0,\r\n",
      "        \"priority_queue_policy\": {}\r\n",
      "    },\r\n",
      "    \"instance_group\": [\r\n",
      "        {\r\n",
      "            \"name\": \"sentiment-nltk-service_0\",\r\n",
      "            \"kind\": \"KIND_CPU\",\r\n",
      "            \"count\": 1,\r\n",
      "            \"gpus\": [],\r\n",
      "            \"secondary_devices\": [],\r\n",
      "            \"profile\": [],\r\n",
      "            \"passive\": false,\r\n",
      "            \"host_policy\": \"\"\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"default_model_filename\": \"model.py\",\r\n",
      "    \"cc_model_filenames\": {},\r\n",
      "    \"metric_tags\": {},\r\n",
      "    \"parameters\": {\r\n",
      "        \"EXECUTION_ENV_PATH\": {\r\n",
      "            \"string_value\": \"/mnt/data/model_repository/sentiment-nltk-service/sentinltkenv.tar.gz\"\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"model_warmup\": []\r\n",
      "}\r\n",
      "I0226 05:32:52.011689 1 python_be.cc:1858] TRITONBACKEND_ModelInstanceInitialize: sentiment-nltk-service_0 (CPU device 0)\r\n",
      "I0226 05:32:52.011711 1 backend_model_instance.cc:68] Creating instance sentiment-nltk-service_0 on CPU using artifact 'model.py'\r\n",
      "I0226 05:32:52.032423 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_IhVPpg/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_IhVPpg/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-nltk-service/1/model.py triton_python_backend_shm_region_2 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-nltk-service_0\r\n",
      "I0226 05:32:52.455495 1 python_be.cc:1879] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful sentiment-nltk-service_0 (device 0)\r\n",
      "I0226 05:32:52.455615 1 backend_model_instance.cc:766] Starting backend thread for sentiment-nltk-service_0 at nice 0 on device 0...\r\n",
      "I0226 05:32:52.457531 1 model_lifecycle.cc:694] successfully loaded 'sentiment-nltk-service' version 1\r\n",
      "I0226 05:32:52.457560 1 dynamic_batch_scheduler.cc:284] Starting dynamic-batcher thread for sentiment-nltk-service at nice 0...\r\n",
      "I0226 05:32:52.458228 1 server.cc:563] \r\n",
      "+------------------+------+\r\n",
      "| Repository Agent | Path |\r\n",
      "+------------------+------+\r\n",
      "+------------------+------+\r\n",
      "\r\n",
      "I0226 05:32:52.458290 1 server.cc:590] \r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Backend | Path                                                  | Config                                                                                                                                                        |\r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| python  | /opt/tritonserver/backends/python/libtriton_python.so | {\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0226 05:32:52.458330 1 server.cc:633] \r\n",
      "+------------------------+---------+--------+\r\n",
      "| Model                  | Version | Status |\r\n",
      "+------------------------+---------+--------+\r\n",
      "| sentiment-nltk-service | 1       | READY  |\r\n",
      "+------------------------+---------+--------+\r\n",
      "\r\n",
      "I0226 05:32:52.459007 1 metrics.cc:757] Collecting CPU metrics\r\n",
      "I0226 05:32:52.459182 1 tritonserver.cc:2264] \r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Option                           | Value                                                                                                                                                                                                |\r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| server_id                        | triton                                                                                                                                                                                               |\r\n",
      "| server_version                   | 2.30.0                                                                                                                                                                                               |\r\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace logging |\r\n",
      "| model_repository_path[0]         | /mnt/data/model_repository                                                                                                                                                                           |\r\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                            |\r\n",
      "| strict_model_config              | 0                                                                                                                                                                                                    |\r\n",
      "| rate_limit                       | OFF                                                                                                                                                                                                  |\r\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                            |\r\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                                    |\r\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                                  |\r\n",
      "| strict_readiness                 | 1                                                                                                                                                                                                    |\r\n",
      "| exit_timeout                     | 30                                                                                                                                                                                                   |\r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0226 05:32:52.587970 1 grpc_server.cc:4763] === GRPC KeepAlive Options ===\r\n",
      "I0226 05:32:52.587995 1 grpc_server.cc:4764] keepalive_time_ms: 7200000\r\n",
      "I0226 05:32:52.588001 1 grpc_server.cc:4766] keepalive_timeout_ms: 20000\r\n",
      "I0226 05:32:52.588009 1 grpc_server.cc:4768] keepalive_permit_without_calls: 0\r\n",
      "I0226 05:32:52.588014 1 grpc_server.cc:4770] http2_max_pings_without_data: 2\r\n",
      "I0226 05:32:52.588018 1 grpc_server.cc:4772] http2_min_recv_ping_interval_without_data_ms: 300000\r\n",
      "I0226 05:32:52.588023 1 grpc_server.cc:4775] http2_max_ping_strikes: 2\r\n",
      "I0226 05:32:52.588028 1 grpc_server.cc:4777] ==============================\r\n",
      "I0226 05:32:52.597361 1 grpc_server.cc:225] Ready for RPC 'ServerLive', 0\r\n",
      "I0226 05:32:52.597393 1 grpc_server.cc:225] Ready for RPC 'ServerReady', 0\r\n",
      "I0226 05:32:52.597402 1 grpc_server.cc:225] Ready for RPC 'ModelReady', 0\r\n",
      "I0226 05:32:52.597410 1 grpc_server.cc:225] Ready for RPC 'ServerMetadata', 0\r\n",
      "I0226 05:32:52.597424 1 grpc_server.cc:225] Ready for RPC 'ModelMetadata', 0\r\n",
      "I0226 05:32:52.597433 1 grpc_server.cc:225] Ready for RPC 'ModelConfig', 0\r\n",
      "I0226 05:32:52.597439 1 grpc_server.cc:225] Ready for RPC 'ModelStatistics', 0\r\n",
      "I0226 05:32:52.597944 1 grpc_server.cc:225] Ready for RPC 'Trace', 0\r\n",
      "I0226 05:32:52.597964 1 grpc_server.cc:225] Ready for RPC 'Logging', 0\r\n",
      "I0226 05:32:52.597994 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryStatus', 0\r\n",
      "I0226 05:32:52.598005 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryRegister', 0\r\n",
      "I0226 05:32:52.598015 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryUnregister', 0\r\n",
      "I0226 05:32:52.598042 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryStatus', 0\r\n",
      "I0226 05:32:52.598055 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryRegister', 0\r\n",
      "I0226 05:32:52.598062 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryUnregister', 0\r\n",
      "I0226 05:32:52.598087 1 grpc_server.cc:225] Ready for RPC 'RepositoryIndex', 0\r\n",
      "I0226 05:32:52.598099 1 grpc_server.cc:225] Ready for RPC 'RepositoryModelLoad', 0\r\n",
      "I0226 05:32:52.598108 1 grpc_server.cc:225] Ready for RPC 'RepositoryModelUnload', 0\r\n",
      "I0226 05:32:52.598166 1 grpc_server.cc:419] Thread started for CommonHandler\r\n",
      "I0226 05:32:52.599008 1 grpc_server.cc:3793] New request handler for ModelInferHandler, 0\r\n",
      "I0226 05:32:52.599058 1 grpc_server.cc:2717] Thread started for ModelInferHandler\r\n",
      "I0226 05:32:52.599193 1 grpc_server.cc:3793] New request handler for ModelInferHandler, 0\r\n",
      "I0226 05:32:52.599237 1 grpc_server.cc:2717] Thread started for ModelInferHandler\r\n",
      "I0226 05:32:52.599374 1 grpc_server.cc:4189] New request handler for ModelStreamInferHandler, 0\r\n",
      "I0226 05:32:52.599413 1 grpc_server.cc:2717] Thread started for ModelStreamInferHandler\r\n",
      "I0226 05:32:52.599422 1 grpc_server.cc:4819] Started GRPCInferenceService at 0.0.0.0:8001\r\n",
      "I0226 05:32:52.607032 1 http_server.cc:3477] Started HTTPService at 0.0.0.0:8000\r\n",
      "I0226 05:32:52.648819 1 http_server.cc:184] Started Metrics Service at 0.0.0.0:8002\r\n"
     ]
    }
   ],
   "source": [
    "!docker logs {container_id[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6cd360",
   "metadata": {},
   "source": [
    "## 4. Inference requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9abc8",
   "metadata": {},
   "source": [
    "### 4.1 Using Python Requests to send HTTP call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68fcffb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'sentiment-nltk-service',\n",
       " 'model_version': '1',\n",
       " 'outputs': [{'name': 'SCORE',\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [1],\n",
       "   'data': ['0.6249']},\n",
       "  {'name': 'STATUS', 'datatype': 'BYTES', 'shape': [1], 'data': ['Success']}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8000/v2/models/sentiment-nltk-service/versions/1/infer\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"inputs\": [\n",
    "    {\n",
    "      \"name\": \"TEXT\",\n",
    "      \"shape\": [\n",
    "        1,\n",
    "        1\n",
    "      ],\n",
    "      \"datatype\": \"BYTES\",\n",
    "      \"data\": [\n",
    "        \"Awesome\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd63b17",
   "metadata": {},
   "source": [
    "### 4.2 Using Python SDK to send GRPC call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3008d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "class SentimentNLTKService:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"sentiment-nltk-service\"\n",
    "        self.input_meta = (\"TEXT\", [1,1], \"BYTES\")\n",
    "        self.output_fields = ['STATUS', 'SCORE']\n",
    "    def grpc_infer_call(self, in_text):\n",
    "        # 1. Client Initialisation\n",
    "        triton_client = grpcclient.InferenceServerClient(url=\"localhost:8001\", verbose=True)\n",
    "        # 2. Input\n",
    "        input_data = grpcclient.InferInput(*self.input_meta)\n",
    "        input_text = np.array([in_text], dtype=object)\n",
    "        input_text.shape = (1,1)\n",
    "        input_data.set_data_from_numpy(input_text)\n",
    "        inputs = [input_data]\n",
    "        # 3. Outputs\n",
    "        outputs = [grpcclient.InferRequestedOutput(field) for field in self.output_fields]\n",
    "        # 4. Send request\n",
    "        results = triton_client.infer(model_name=self.model_name,inputs=inputs,outputs=outputs)\n",
    "        # 5. Return output\n",
    "        return [{field: results.as_numpy(field)} for field in self.output_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d43524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer, metadata ()\n",
      "model_name: \"sentiment-nltk-service\"\n",
      "inputs {\n",
      "  name: \"TEXT\"\n",
      "  datatype: \"BYTES\"\n",
      "  shape: 1\n",
      "  shape: 1\n",
      "}\n",
      "outputs {\n",
      "  name: \"STATUS\"\n",
      "}\n",
      "outputs {\n",
      "  name: \"SCORE\"\n",
      "}\n",
      "raw_input_contents: \"\\007\\000\\000\\000Awesome\"\n",
      "\n",
      "model_name: \"sentiment-nltk-service\"\n",
      "model_version: \"1\"\n",
      "outputs {\n",
      "  name: \"SCORE\"\n",
      "  datatype: \"BYTES\"\n",
      "  shape: 1\n",
      "}\n",
      "outputs {\n",
      "  name: \"STATUS\"\n",
      "  datatype: \"BYTES\"\n",
      "  shape: 1\n",
      "}\n",
      "raw_output_contents: \"\\006\\000\\000\\0000.6249\"\n",
      "raw_output_contents: \"\\007\\000\\000\\000Success\"\n",
      "\n",
      "[{'STATUS': array([b'Success'], dtype=object)}, {'SCORE': array([b'0.6249'], dtype=object)}]\n"
     ]
    }
   ],
   "source": [
    "sentiment_service_client = SentimentNLTKService()\n",
    "print(sentiment_service_client.grpc_infer_call(\"Awesome\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470646d5",
   "metadata": {},
   "source": [
    "## 5. Health and Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0ed15",
   "metadata": {},
   "source": [
    "### 5.1. Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c40dca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "\n",
    "triton_client = httpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b5d88ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ad8e25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/ready, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.is_server_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e8f9998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/models/sentiment-nltk-service/ready, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.is_model_ready(\"sentiment-nltk-service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9adcb788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/models/sentiment-nltk-service, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '245'}>\n",
      "bytearray(b'{\"name\":\"sentiment-nltk-service\",\"versions\":[\"1\"],\"platform\":\"python\",\"inputs\":[{\"name\":\"TEXT\",\"datatype\":\"BYTES\",\"shape\":[-1,1]}],\"outputs\":[{\"name\":\"STATUS\",\"datatype\":\"BYTES\",\"shape\":[-1,1]},{\"name\":\"SCORE\",\"datatype\":\"FP32\",\"shape\":[-1,1]}]}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'sentiment-nltk-service',\n",
       " 'versions': ['1'],\n",
       " 'platform': 'python',\n",
       " 'inputs': [{'name': 'TEXT', 'datatype': 'BYTES', 'shape': [-1, 1]}],\n",
       " 'outputs': [{'name': 'STATUS', 'datatype': 'BYTES', 'shape': [-1, 1]},\n",
       "  {'name': 'SCORE', 'datatype': 'FP32', 'shape': [-1, 1]}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_metadata(\"sentiment-nltk-service\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d25f11",
   "metadata": {},
   "source": [
    "### 5.2. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddaa0f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/models/stats, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '592'}>\n",
      "bytearray(b'{\"model_stats\":[{\"name\":\"sentiment-nltk-service\",\"version\":\"1\",\"last_inference\":1677389751622,\"inference_count\":2,\"execution_count\":2,\"inference_stats\":{\"success\":{\"count\":2,\"ns\":16411202},\"fail\":{\"count\":0,\"ns\":0},\"queue\":{\"count\":2,\"ns\":566035},\"compute_input\":{\"count\":2,\"ns\":402930},\"compute_infer\":{\"count\":2,\"ns\":14881326},\"compute_output\":{\"count\":2,\"ns\":538076},\"cache_hit\":{\"count\":0,\"ns\":0},\"cache_miss\":{\"count\":0,\"ns\":0}},\"batch_stats\":[{\"batch_size\":1,\"compute_input\":{\"count\":2,\"ns\":402930},\"compute_infer\":{\"count\":2,\"ns\":14881326},\"compute_output\":{\"count\":2,\"ns\":538076}}]}]}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_stats': [{'name': 'sentiment-nltk-service',\n",
       "   'version': '1',\n",
       "   'last_inference': 1677389751622,\n",
       "   'inference_count': 2,\n",
       "   'execution_count': 2,\n",
       "   'inference_stats': {'success': {'count': 2, 'ns': 16411202},\n",
       "    'fail': {'count': 0, 'ns': 0},\n",
       "    'queue': {'count': 2, 'ns': 566035},\n",
       "    'compute_input': {'count': 2, 'ns': 402930},\n",
       "    'compute_infer': {'count': 2, 'ns': 14881326},\n",
       "    'compute_output': {'count': 2, 'ns': 538076},\n",
       "    'cache_hit': {'count': 0, 'ns': 0},\n",
       "    'cache_miss': {'count': 0, 'ns': 0}},\n",
       "   'batch_stats': [{'batch_size': 1,\n",
       "     'compute_input': {'count': 2, 'ns': 402930},\n",
       "     'compute_infer': {'count': 2, 'ns': 14881326},\n",
       "     'compute_output': {'count': 2, 'ns': 538076}}]}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_inference_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fb87011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HELP nv_inference_request_success Number of successful inference requests, all batch sizes\n",
      "# TYPE nv_inference_request_success counter\n",
      "nv_inference_request_success{model=\"sentiment-nltk-service\",version=\"1\"} 2\n",
      "# HELP nv_inference_request_failure Number of failed inference requests, all batch sizes\n",
      "# TYPE nv_inference_request_failure counter\n",
      "nv_inference_request_failure{model=\"sentiment-nltk-service\",version=\"1\"} 0\n",
      "# HELP nv_inference_count Number of inferences performed (does not include cached requests)\n",
      "# TYPE nv_inference_count counter\n",
      "nv_inference_count{model=\"sentiment-nltk-service\",version=\"1\"} 2\n",
      "# HELP nv_inference_exec_count Number of model executions performed (does not include cached requests)\n",
      "# TYPE nv_inference_exec_count counter\n",
      "nv_inference_exec_count{model=\"sentiment-nltk-service\",version=\"1\"} 2\n",
      "# HELP nv_inference_request_duration_us Cumulative inference request duration in microseconds (includes cached requests)\n",
      "# TYPE nv_inference_request_duration_us counter\n",
      "nv_inference_request_duration_us{model=\"sentiment-nltk-service\",version=\"1\"} 16411\n",
      "# HELP nv_inference_queue_duration_us Cumulative inference queuing duration in microseconds (includes cached requests)\n",
      "# TYPE nv_inference_queue_duration_us counter\n",
      "nv_inference_queue_duration_us{model=\"sentiment-nltk-service\",version=\"1\"} 565\n",
      "# HELP nv_inference_compute_input_duration_us Cumulative compute input duration in microseconds (does not include cached requests)\n",
      "# TYPE nv_inference_compute_input_duration_us counter\n",
      "nv_inference_compute_input_duration_us{model=\"sentiment-nltk-service\",version=\"1\"} 402\n",
      "# HELP nv_inference_compute_infer_duration_us Cumulative compute inference duration in microseconds (does not include cached requests)\n",
      "# TYPE nv_inference_compute_infer_duration_us counter\n",
      "nv_inference_compute_infer_duration_us{model=\"sentiment-nltk-service\",version=\"1\"} 14880\n",
      "# HELP nv_inference_compute_output_duration_us Cumulative inference compute output duration in microseconds (does not include cached requests)\n",
      "# TYPE nv_inference_compute_output_duration_us counter\n",
      "nv_inference_compute_output_duration_us{model=\"sentiment-nltk-service\",version=\"1\"} 537\n",
      "# HELP nv_cache_num_hits_per_model Number of cache hits per model\n",
      "# TYPE nv_cache_num_hits_per_model counter\n",
      "nv_cache_num_hits_per_model{model=\"sentiment-nltk-service\",version=\"1\"} 0\n",
      "# HELP nv_cache_hit_lookup_duration_per_model Total cache hit lookup duration per model, in microseconds\n",
      "# TYPE nv_cache_hit_lookup_duration_per_model counter\n",
      "nv_cache_hit_lookup_duration_per_model{model=\"sentiment-nltk-service\",version=\"1\"} 0\n",
      "# HELP nv_cache_num_misses_per_model Number of cache misses per model\n",
      "# TYPE nv_cache_num_misses_per_model counter\n",
      "nv_cache_num_misses_per_model{model=\"sentiment-nltk-service\",version=\"1\"} 0\n",
      "# HELP nv_cache_miss_lookup_duration_per_model Total cache miss lookup duration per model, in microseconds\n",
      "# TYPE nv_cache_miss_lookup_duration_per_model counter\n",
      "nv_cache_miss_lookup_duration_per_model{model=\"sentiment-nltk-service\",version=\"1\"} 0\n",
      "# HELP nv_cache_miss_insertion_duration_per_model Total cache miss insertion duration per model, in microseconds\n",
      "# TYPE nv_cache_miss_insertion_duration_per_model counter\n",
      "nv_cache_miss_insertion_duration_per_model{model=\"sentiment-nltk-service\",version=\"1\"} 0\n",
      "# HELP nv_cpu_utilization CPU utilization rate [0.0 - 1.0]\n",
      "# TYPE nv_cpu_utilization gauge\n",
      "nv_cpu_utilization 0.0025\n",
      "# HELP nv_cpu_memory_total_bytes CPU total memory (RAM), in bytes\n",
      "# TYPE nv_cpu_memory_total_bytes gauge\n",
      "nv_cpu_memory_total_bytes 89628459008\n",
      "# HELP nv_cpu_memory_used_bytes CPU used memory (RAM), in bytes\n",
      "# TYPE nv_cpu_memory_used_bytes gauge\n",
      "nv_cpu_memory_used_bytes 1836240896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8002/metrics\"\n",
    "\n",
    "payload={}\n",
    "headers = {}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90b9c1",
   "metadata": {},
   "source": [
    "## 6. Cleanups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdec3f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6099717b6c56a70498a528171a34faf4b9cc776dc14851d36b3da19ee0cf876b\r\n"
     ]
    }
   ],
   "source": [
    "!docker stop {container_id[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "947fbdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! This will remove all stopped containers.\r\n",
      "Are you sure you want to continue? [y/N] Deleted Containers:\r\n",
      "6099717b6c56a70498a528171a34faf4b9cc776dc14851d36b3da19ee0cf876b\r\n",
      "\r\n",
      "Total reclaimed space: 12.55kB\r\n"
     ]
    }
   ],
   "source": [
    "!echo y | docker container prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967acd31",
   "metadata": {},
   "source": [
    "# II. Non-Optimisation Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746216a6",
   "metadata": {},
   "source": [
    "## 1. Multi-Versions support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5535776e",
   "metadata": {},
   "source": [
    "### 1.1. Code repository structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a936a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./model_repository_multi_version\u001b[00m\r\n",
      "└── \u001b[01;34msentiment-service\u001b[00m\r\n",
      "    ├── \u001b[01;34m1\u001b[00m\r\n",
      "    │   ├── __init__.py\r\n",
      "    │   ├── model.py\r\n",
      "    │   └── \u001b[01;34mresources\u001b[00m\r\n",
      "    │       └── \u001b[01;34mnltk\u001b[00m\r\n",
      "    │           └── \u001b[01;34msentiment\u001b[00m\r\n",
      "    │               └── \u001b[01;31mvader_lexicon.zip\u001b[00m\r\n",
      "    ├── \u001b[01;34m2\u001b[00m\r\n",
      "    │   ├── __init__.py\r\n",
      "    │   ├── model.py\r\n",
      "    │   └── \u001b[01;34mresources\u001b[00m\r\n",
      "    │       └── \u001b[01;34mspacy\u001b[00m\r\n",
      "    │           └── \u001b[01;34men_core_web_sm-3.3.0\u001b[00m\r\n",
      "    │               ├── LICENSE\r\n",
      "    │               ├── LICENSES_SOURCES\r\n",
      "    │               ├── README.md\r\n",
      "    │               ├── accuracy.json\r\n",
      "    │               ├── \u001b[01;34mattribute_ruler\u001b[00m\r\n",
      "    │               │   └── patterns\r\n",
      "    │               ├── config.cfg\r\n",
      "    │               ├── \u001b[01;34mlemmatizer\u001b[00m\r\n",
      "    │               │   └── \u001b[01;34mlookups\u001b[00m\r\n",
      "    │               │       └── lookups.bin\r\n",
      "    │               ├── meta.json\r\n",
      "    │               ├── \u001b[01;34mner\u001b[00m\r\n",
      "    │               │   ├── cfg\r\n",
      "    │               │   ├── model\r\n",
      "    │               │   └── moves\r\n",
      "    │               ├── \u001b[01;34mparser\u001b[00m\r\n",
      "    │               │   ├── cfg\r\n",
      "    │               │   ├── model\r\n",
      "    │               │   └── moves\r\n",
      "    │               ├── \u001b[01;34msenter\u001b[00m\r\n",
      "    │               │   ├── cfg\r\n",
      "    │               │   └── model\r\n",
      "    │               ├── \u001b[01;34mtagger\u001b[00m\r\n",
      "    │               │   ├── cfg\r\n",
      "    │               │   └── model\r\n",
      "    │               ├── \u001b[01;34mtok2vec\u001b[00m\r\n",
      "    │               │   ├── cfg\r\n",
      "    │               │   └── model\r\n",
      "    │               ├── tokenizer\r\n",
      "    │               └── \u001b[01;34mvocab\u001b[00m\r\n",
      "    │                   ├── key2row\r\n",
      "    │                   ├── lookups.bin\r\n",
      "    │                   ├── strings.json\r\n",
      "    │                   ├── vectors\r\n",
      "    │                   └── vectors.cfg\r\n",
      "    ├── \u001b[01;32mbuild_env.sh\u001b[00m\r\n",
      "    ├── config.pbtxt\r\n",
      "    ├── requirements.txt\r\n",
      "    └── \u001b[01;31msentimentenv.tar.gz\u001b[00m\r\n",
      "\r\n",
      "18 directories, 35 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ./model_repository_multi_version -I '__pycache__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b286ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"sentiment-service\"\r\n",
      "backend: \"python\"\r\n",
      "max_batch_size: 8\r\n",
      "\r\n",
      "dynamic_batching { }\r\n",
      "\r\n",
      "input [\r\n",
      "  {\r\n",
      "    name: \"TEXT\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims:  [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "output [\r\n",
      "  {\r\n",
      "    name: \"STATUS\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims: [1]\r\n",
      "  },\r\n",
      "  {\r\n",
      "    name: \"SCORE\"\r\n",
      "    data_type: TYPE_FP32\r\n",
      "    dims: [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "parameters: {\r\n",
      "  key: \"EXECUTION_ENV_PATH\",\r\n",
      "  value: {string_value: \"/mnt/data/model_repository/sentiment-service/sentimentenv.tar.gz\"}\r\n",
      "}\r\n",
      "\r\n",
      "instance_group [\r\n",
      "  {\r\n",
      "    count: 1\r\n",
      "    kind: KIND_CPU\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "version_policy: { all: {}}\r\n",
      "# Other possible values:\r\n",
      "# version_policy: { latest: { num_versions: 2}}\r\n",
      "# version_policy: { specific: { versions: [1,2]}} \r\n"
     ]
    }
   ],
   "source": [
    "!cat model_repository_multi_version/sentiment-service/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bade59",
   "metadata": {},
   "source": [
    "### 1.2 Run container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eeb62cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_id=!(docker run -d \\\n",
    "                --shm-size=5G \\\n",
    "                -p8000:8000 -p8001:8001 -p8002:8002 \\\n",
    "                -v $PWD/model_repository_multi_version:/mnt/data/model_repository \\\n",
    "                nvcr.io/nvidia/tritonserver:23.01-py3 \\\n",
    "                tritonserver \\\n",
    "                --model-repository=/mnt/data/model_repository \\\n",
    "                --log-verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fae52d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5cd70b4d91e8a2577bc43afae724703921b57c2affdd04c828ba31519c730a1f']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ff02b82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "=============================\r\n",
      "== Triton Inference Server ==\r\n",
      "=============================\r\n",
      "\r\n",
      "NVIDIA Release 23.01 (build 52277748)\r\n",
      "Triton Server Version 2.30.0\r\n",
      "\r\n",
      "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\r\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n",
      "\r\n",
      "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n",
      "   Use the NVIDIA Container Toolkit to start this container with GPU support; see\r\n",
      "   https://docs.nvidia.com/datacenter/cloud-native/ .\r\n",
      "\r\n",
      "W0226 05:44:36.503505 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\n",
      "I0226 05:44:36.503652 1 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\n",
      "I0226 05:44:36.506286 1 model_config_utils.cc:646] Server side auto-completed config: name: \"sentiment-service\"\r\n",
      "version_policy {\r\n",
      "  all {\r\n",
      "  }\r\n",
      "}\r\n",
      "max_batch_size: 8\r\n",
      "input {\r\n",
      "  name: \"TEXT\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "output {\r\n",
      "  name: \"STATUS\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "output {\r\n",
      "  name: \"SCORE\"\r\n",
      "  data_type: TYPE_FP32\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "instance_group {\r\n",
      "  count: 1\r\n",
      "  kind: KIND_CPU\r\n",
      "}\r\n",
      "default_model_filename: \"model.py\"\r\n",
      "dynamic_batching {\r\n",
      "}\r\n",
      "parameters {\r\n",
      "  key: \"EXECUTION_ENV_PATH\"\r\n",
      "  value {\r\n",
      "    string_value: \"/mnt/data/model_repository/sentiment-service/sentimentenv.tar.gz\"\r\n",
      "  }\r\n",
      "}\r\n",
      "backend: \"python\"\r\n",
      "\r\n",
      "I0226 05:44:36.506395 1 model_lifecycle.cc:459] loading: sentiment-service:1\r\n",
      "I0226 05:44:36.506447 1 model_lifecycle.cc:459] loading: sentiment-service:2\r\n",
      "I0226 05:44:36.506571 1 backend_model.cc:348] Adding default backend config setting: default-max-batch-size,4\r\n",
      "I0226 05:44:36.506604 1 backend_model.cc:348] Adding default backend config setting: default-max-batch-size,4\r\n",
      "I0226 05:44:36.506609 1 shared_library.cc:112] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\r\n",
      "I0226 05:44:36.508458 1 python_be.cc:1614] 'python' TRITONBACKEND API version: 1.11\r\n",
      "I0226 05:44:36.508479 1 python_be.cc:1636] backend configuration:\r\n",
      "{\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\r\n",
      "I0226 05:44:36.508522 1 python_be.cc:1766] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\r\n",
      "I0226 05:44:36.508676 1 python_be.cc:2012] TRITONBACKEND_GetBackendAttribute: setting attributes\r\n",
      "I0226 05:44:36.508713 1 python_be.cc:1814] TRITONBACKEND_ModelInitialize: sentiment-service (version 1)\r\n",
      "I0226 05:44:36.509186 1 model_config_utils.cc:1838] ModelConfig 64-bit fields:\r\n",
      "I0226 05:44:36.509201 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\r\n",
      "I0226 05:44:36.509206 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\r\n",
      "I0226 05:44:36.509211 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\r\n",
      "I0226 05:44:36.509215 1 model_config_utils.cc:1840] \tModelConfig::ensemble_scheduling::step::model_version\r\n",
      "I0226 05:44:36.509220 1 model_config_utils.cc:1840] \tModelConfig::input::dims\r\n",
      "I0226 05:44:36.509225 1 model_config_utils.cc:1840] \tModelConfig::input::reshape::shape\r\n",
      "I0226 05:44:36.509230 1 model_config_utils.cc:1840] \tModelConfig::instance_group::secondary_devices::device_id\r\n",
      "I0226 05:44:36.509235 1 model_config_utils.cc:1840] \tModelConfig::model_warmup::inputs::value::dims\r\n",
      "I0226 05:44:36.509239 1 model_config_utils.cc:1840] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\r\n",
      "I0226 05:44:36.509244 1 model_config_utils.cc:1840] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\r\n",
      "I0226 05:44:36.509250 1 model_config_utils.cc:1840] \tModelConfig::output::dims\r\n",
      "I0226 05:44:36.509255 1 model_config_utils.cc:1840] \tModelConfig::output::reshape::shape\r\n",
      "I0226 05:44:36.509260 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\r\n",
      "I0226 05:44:36.509265 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\r\n",
      "I0226 05:44:36.509270 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\r\n",
      "I0226 05:44:36.509275 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::state::dims\r\n",
      "I0226 05:44:36.509280 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::state::initial_state::dims\r\n",
      "I0226 05:44:36.509284 1 model_config_utils.cc:1840] \tModelConfig::version_policy::specific::versions\r\n",
      "I0226 05:44:36.509418 1 python_be.cc:1505] Using Python execution env /mnt/data/model_repository/sentiment-service/sentimentenv.tar.gz\r\n",
      "I0226 05:44:40.370441 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_r69bDi/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_r69bDi/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-service/1/model.py triton_python_backend_shm_region_1 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-service\r\n",
      "I0226 05:44:41.958758 1 python_be.cc:1594] model configuration:\r\n",
      "{\r\n",
      "    \"name\": \"sentiment-service\",\r\n",
      "    \"platform\": \"\",\r\n",
      "    \"backend\": \"python\",\r\n",
      "    \"version_policy\": {\r\n",
      "        \"all\": {}\r\n",
      "    },\r\n",
      "    \"max_batch_size\": 8,\r\n",
      "    \"input\": [\r\n",
      "        {\r\n",
      "            \"name\": \"TEXT\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"format\": \"FORMAT_NONE\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"is_shape_tensor\": false,\r\n",
      "            \"allow_ragged_batch\": false,\r\n",
      "            \"optional\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"output\": [\r\n",
      "        {\r\n",
      "            \"name\": \"STATUS\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"SCORE\",\r\n",
      "            \"data_type\": \"TYPE_FP32\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"batch_input\": [],\r\n",
      "    \"batch_output\": [],\r\n",
      "    \"optimization\": {\r\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\r\n",
      "        \"input_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"output_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"gather_kernel_buffer_threshold\": 0,\r\n",
      "        \"eager_batching\": false\r\n",
      "    },\r\n",
      "    \"dynamic_batching\": {\r\n",
      "        \"preferred_batch_size\": [\r\n",
      "            8\r\n",
      "        ],\r\n",
      "        \"max_queue_delay_microseconds\": 0,\r\n",
      "        \"preserve_ordering\": false,\r\n",
      "        \"priority_levels\": 0,\r\n",
      "        \"default_priority_level\": 0,\r\n",
      "        \"priority_queue_policy\": {}\r\n",
      "    },\r\n",
      "    \"instance_group\": [\r\n",
      "        {\r\n",
      "            \"name\": \"sentiment-service_0\",\r\n",
      "            \"kind\": \"KIND_CPU\",\r\n",
      "            \"count\": 1,\r\n",
      "            \"gpus\": [],\r\n",
      "            \"secondary_devices\": [],\r\n",
      "            \"profile\": [],\r\n",
      "            \"passive\": false,\r\n",
      "            \"host_policy\": \"\"\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"default_model_filename\": \"model.py\",\r\n",
      "    \"cc_model_filenames\": {},\r\n",
      "    \"metric_tags\": {},\r\n",
      "    \"parameters\": {\r\n",
      "        \"EXECUTION_ENV_PATH\": {\r\n",
      "            \"string_value\": \"/mnt/data/model_repository/sentiment-service/sentimentenv.tar.gz\"\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"model_warmup\": []\r\n",
      "}\r\n",
      "I0226 05:44:41.958863 1 python_be.cc:1814] TRITONBACKEND_ModelInitialize: sentiment-service (version 2)\r\n",
      "I0226 05:44:41.959368 1 python_be.cc:1505] Using Python execution env /mnt/data/model_repository/sentiment-service/sentimentenv.tar.gz\r\n",
      "I0226 05:44:41.980913 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_r69bDi/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_r69bDi/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-service/2/model.py triton_python_backend_shm_region_2 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-service\r\n",
      "I0226 05:44:44.217821 1 python_be.cc:1594] model configuration:\r\n",
      "{\r\n",
      "    \"name\": \"sentiment-service\",\r\n",
      "    \"platform\": \"\",\r\n",
      "    \"backend\": \"python\",\r\n",
      "    \"version_policy\": {\r\n",
      "        \"all\": {}\r\n",
      "    },\r\n",
      "    \"max_batch_size\": 8,\r\n",
      "    \"input\": [\r\n",
      "        {\r\n",
      "            \"name\": \"TEXT\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"format\": \"FORMAT_NONE\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"is_shape_tensor\": false,\r\n",
      "            \"allow_ragged_batch\": false,\r\n",
      "            \"optional\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"output\": [\r\n",
      "        {\r\n",
      "            \"name\": \"STATUS\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"SCORE\",\r\n",
      "            \"data_type\": \"TYPE_FP32\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"batch_input\": [],\r\n",
      "    \"batch_output\": [],\r\n",
      "    \"optimization\": {\r\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\r\n",
      "        \"input_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"output_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"gather_kernel_buffer_threshold\": 0,\r\n",
      "        \"eager_batching\": false\r\n",
      "    },\r\n",
      "    \"dynamic_batching\": {\r\n",
      "        \"preferred_batch_size\": [\r\n",
      "            8\r\n",
      "        ],\r\n",
      "        \"max_queue_delay_microseconds\": 0,\r\n",
      "        \"preserve_ordering\": false,\r\n",
      "        \"priority_levels\": 0,\r\n",
      "        \"default_priority_level\": 0,\r\n",
      "        \"priority_queue_policy\": {}\r\n",
      "    },\r\n",
      "    \"instance_group\": [\r\n",
      "        {\r\n",
      "            \"name\": \"sentiment-service_0\",\r\n",
      "            \"kind\": \"KIND_CPU\",\r\n",
      "            \"count\": 1,\r\n",
      "            \"gpus\": [],\r\n",
      "            \"secondary_devices\": [],\r\n",
      "            \"profile\": [],\r\n",
      "            \"passive\": false,\r\n",
      "            \"host_policy\": \"\"\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"default_model_filename\": \"model.py\",\r\n",
      "    \"cc_model_filenames\": {},\r\n",
      "    \"metric_tags\": {},\r\n",
      "    \"parameters\": {\r\n",
      "        \"EXECUTION_ENV_PATH\": {\r\n",
      "            \"string_value\": \"/mnt/data/model_repository/sentiment-service/sentimentenv.tar.gz\"\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"model_warmup\": []\r\n",
      "}\r\n",
      "I0226 05:44:44.217997 1 python_be.cc:1858] TRITONBACKEND_ModelInstanceInitialize: sentiment-service_0 (CPU device 0)\r\n",
      "I0226 05:44:44.218052 1 backend_model_instance.cc:68] Creating instance sentiment-service_0 on CPU using artifact 'model.py'\r\n",
      "I0226 05:44:44.240293 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_r69bDi/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_r69bDi/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-service/1/model.py triton_python_backend_shm_region_3 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-service_0\r\n",
      "I0226 05:44:44.674038 1 python_be.cc:1879] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful sentiment-service_0 (device 0)\r\n",
      "I0226 05:44:44.674158 1 python_be.cc:1858] TRITONBACKEND_ModelInstanceInitialize: sentiment-service_0 (CPU device 0)\r\n",
      "I0226 05:44:44.674186 1 backend_model_instance.cc:68] Creating instance sentiment-service_0 on CPU using artifact 'model.py'\r\n",
      "I0226 05:44:44.674205 1 backend_model_instance.cc:766] Starting backend thread for sentiment-service_0 at nice 0 on device 0...\r\n",
      "I0226 05:44:44.674476 1 dynamic_batch_scheduler.cc:284] Starting dynamic-batcher thread for sentiment-service at nice 0...\r\n",
      "I0226 05:44:44.696005 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_r69bDi/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_r69bDi/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-service/2/model.py triton_python_backend_shm_region_4 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-service_0\r\n",
      "I0226 05:44:46.966772 1 python_be.cc:1879] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful sentiment-service_0 (device 0)\r\n",
      "I0226 05:44:46.966940 1 backend_model_instance.cc:766] Starting backend thread for sentiment-service_0 at nice 0 on device 0...\r\n",
      "I0226 05:44:46.967121 1 model_lifecycle.cc:694] successfully loaded 'sentiment-service' version 2\r\n",
      "I0226 05:44:46.967122 1 dynamic_batch_scheduler.cc:284] Starting dynamic-batcher thread for sentiment-service at nice 0...\r\n",
      "I0226 05:44:46.967144 1 model_lifecycle.cc:694] successfully loaded 'sentiment-service' version 2\r\n",
      "I0226 05:44:46.967348 1 server.cc:563] \r\n",
      "+------------------+------+\r\n",
      "| Repository Agent | Path |\r\n",
      "+------------------+------+\r\n",
      "+------------------+------+\r\n",
      "\r\n",
      "I0226 05:44:46.967409 1 server.cc:590] \r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Backend | Path                                                  | Config                                                                                                                                                        |\r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| python  | /opt/tritonserver/backends/python/libtriton_python.so | {\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0226 05:44:46.967447 1 server.cc:633] \r\n",
      "+-------------------+---------+--------+\r\n",
      "| Model             | Version | Status |\r\n",
      "+-------------------+---------+--------+\r\n",
      "| sentiment-service | 1       | READY  |\r\n",
      "| sentiment-service | 2       | READY  |\r\n",
      "+-------------------+---------+--------+\r\n",
      "\r\n",
      "I0226 05:44:46.967629 1 metrics.cc:757] Collecting CPU metrics\r\n",
      "I0226 05:44:46.967802 1 tritonserver.cc:2264] \r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Option                           | Value                                                                                                                                                                                                |\r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| server_id                        | triton                                                                                                                                                                                               |\r\n",
      "| server_version                   | 2.30.0                                                                                                                                                                                               |\r\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace logging |\r\n",
      "| model_repository_path[0]         | /mnt/data/model_repository                                                                                                                                                                           |\r\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                            |\r\n",
      "| strict_model_config              | 0                                                                                                                                                                                                    |\r\n",
      "| rate_limit                       | OFF                                                                                                                                                                                                  |\r\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                            |\r\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                                    |\r\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                                  |\r\n",
      "| strict_readiness                 | 1                                                                                                                                                                                                    |\r\n",
      "| exit_timeout                     | 30                                                                                                                                                                                                   |\r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0226 05:44:46.968382 1 grpc_server.cc:4763] === GRPC KeepAlive Options ===\r\n",
      "I0226 05:44:46.968423 1 grpc_server.cc:4764] keepalive_time_ms: 7200000\r\n",
      "I0226 05:44:46.968431 1 grpc_server.cc:4766] keepalive_timeout_ms: 20000\r\n",
      "I0226 05:44:46.968436 1 grpc_server.cc:4768] keepalive_permit_without_calls: 0\r\n",
      "I0226 05:44:46.968441 1 grpc_server.cc:4770] http2_max_pings_without_data: 2\r\n",
      "I0226 05:44:46.968446 1 grpc_server.cc:4772] http2_min_recv_ping_interval_without_data_ms: 300000\r\n",
      "I0226 05:44:46.968451 1 grpc_server.cc:4775] http2_max_ping_strikes: 2\r\n",
      "I0226 05:44:46.968456 1 grpc_server.cc:4777] ==============================\r\n",
      "I0226 05:44:46.971140 1 grpc_server.cc:225] Ready for RPC 'ServerLive', 0\r\n",
      "I0226 05:44:46.971167 1 grpc_server.cc:225] Ready for RPC 'ServerReady', 0\r\n",
      "I0226 05:44:46.971175 1 grpc_server.cc:225] Ready for RPC 'ModelReady', 0\r\n",
      "I0226 05:44:46.971182 1 grpc_server.cc:225] Ready for RPC 'ServerMetadata', 0\r\n",
      "I0226 05:44:46.971190 1 grpc_server.cc:225] Ready for RPC 'ModelMetadata', 0\r\n",
      "I0226 05:44:46.971215 1 grpc_server.cc:225] Ready for RPC 'ModelConfig', 0\r\n",
      "I0226 05:44:46.971226 1 grpc_server.cc:225] Ready for RPC 'ModelStatistics', 0\r\n",
      "I0226 05:44:46.971238 1 grpc_server.cc:225] Ready for RPC 'Trace', 0\r\n",
      "I0226 05:44:46.971247 1 grpc_server.cc:225] Ready for RPC 'Logging', 0\r\n",
      "I0226 05:44:46.971273 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryStatus', 0\r\n",
      "I0226 05:44:46.971283 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryRegister', 0\r\n",
      "I0226 05:44:46.971291 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryUnregister', 0\r\n",
      "I0226 05:44:46.971316 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryStatus', 0\r\n",
      "I0226 05:44:46.971331 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryRegister', 0\r\n",
      "I0226 05:44:46.971337 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryUnregister', 0\r\n",
      "I0226 05:44:46.971362 1 grpc_server.cc:225] Ready for RPC 'RepositoryIndex', 0\r\n",
      "I0226 05:44:46.971373 1 grpc_server.cc:225] Ready for RPC 'RepositoryModelLoad', 0\r\n",
      "I0226 05:44:46.971388 1 grpc_server.cc:225] Ready for RPC 'RepositoryModelUnload', 0\r\n",
      "I0226 05:44:46.971430 1 grpc_server.cc:419] Thread started for CommonHandler\r\n",
      "I0226 05:44:46.971584 1 grpc_server.cc:3793] New request handler for ModelInferHandler, 0\r\n",
      "I0226 05:44:46.971623 1 grpc_server.cc:2717] Thread started for ModelInferHandler\r\n",
      "I0226 05:44:46.971751 1 grpc_server.cc:3793] New request handler for ModelInferHandler, 0\r\n",
      "I0226 05:44:46.971808 1 grpc_server.cc:2717] Thread started for ModelInferHandler\r\n",
      "I0226 05:44:46.971944 1 grpc_server.cc:4189] New request handler for ModelStreamInferHandler, 0\r\n",
      "I0226 05:44:46.971989 1 grpc_server.cc:2717] Thread started for ModelStreamInferHandler\r\n",
      "I0226 05:44:46.972000 1 grpc_server.cc:4819] Started GRPCInferenceService at 0.0.0.0:8001\r\n",
      "I0226 05:44:46.972271 1 http_server.cc:3477] Started HTTPService at 0.0.0.0:8000\r\n",
      "I0226 05:44:47.013541 1 http_server.cc:184] Started Metrics Service at 0.0.0.0:8002\r\n"
     ]
    }
   ],
   "source": [
    "!docker logs {container_id[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7739d1",
   "metadata": {},
   "source": [
    "### 1.3 Inference Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e3f4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8000/v2/models/sentiment-service/versions/{}/infer\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"inputs\": [\n",
    "    {\n",
    "      \"name\": \"TEXT\",\n",
    "      \"shape\": [\n",
    "        1,\n",
    "        1\n",
    "      ],\n",
    "      \"datatype\": \"BYTES\",\n",
    "      \"data\": [\n",
    "        \"Awesome\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7611c332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- V1 Request ---------------\n",
      "URL:http://localhost:8000/v2/models/sentiment-service/versions/1/infer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'sentiment-service',\n",
       " 'model_version': '1',\n",
       " 'outputs': [{'name': 'SCORE',\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [1],\n",
       "   'data': ['0.6249']},\n",
       "  {'name': 'STATUS', 'datatype': 'BYTES', 'shape': [1], 'data': ['Success']}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--------------- V1 Request ---------------\")\n",
    "versioned_url = url.format(\"1\")\n",
    "print(f\"URL:{versioned_url}\")\n",
    "response = requests.request(\"POST\",versioned_url , headers=headers, data=payload)\n",
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96416580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- V2 Request ---------------\n",
      "URL:http://localhost:8000/v2/models/sentiment-service/versions/2/infer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'sentiment-service',\n",
       " 'model_version': '2',\n",
       " 'outputs': [{'name': 'SCORE',\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [1],\n",
       "   'data': ['1.0']},\n",
       "  {'name': 'STATUS', 'datatype': 'BYTES', 'shape': [1], 'data': ['Success']}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--------------- V2 Request ---------------\")\n",
    "versioned_url = url.format(\"2\")\n",
    "print(f\"URL:{versioned_url}\")\n",
    "response = requests.request(\"POST\",versioned_url , headers=headers, data=payload)\n",
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73803dd",
   "metadata": {},
   "source": [
    "### 1.4 Cleanups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62d53c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5cd70b4d91e8a2577bc43afae724703921b57c2affdd04c828ba31519c730a1f\r\n"
     ]
    }
   ],
   "source": [
    "!docker stop {container_id[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62d24a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! This will remove all stopped containers.\r\n",
      "Are you sure you want to continue? [y/N] Deleted Containers:\r\n",
      "5cd70b4d91e8a2577bc43afae724703921b57c2affdd04c828ba31519c730a1f\r\n",
      "\r\n",
      "Total reclaimed space: 12.55kB\r\n"
     ]
    }
   ],
   "source": [
    "!echo y | docker container prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8832d79c",
   "metadata": {},
   "source": [
    "## 2. GPU support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb76d5",
   "metadata": {},
   "source": [
    "### 2.1. Code repository structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbb2acf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./model_repository_gpu_service\u001b[00m\r\n",
      "└── \u001b[01;34mtranslation-service\u001b[00m\r\n",
      "    ├── \u001b[01;34m1\u001b[00m\r\n",
      "    │   ├── \u001b[01;34mhelpers\u001b[00m\r\n",
      "    │   │   └── translation.py\r\n",
      "    │   ├── model.py\r\n",
      "    │   └── \u001b[01;34mresources\u001b[00m\r\n",
      "    │       ├── __init__.py\r\n",
      "    │       └── \u001b[01;34mm2m100_418M\u001b[00m\r\n",
      "    │           ├── README.md\r\n",
      "    │           ├── __init__.py\r\n",
      "    │           ├── config.json\r\n",
      "    │           ├── pytorch_model.bin\r\n",
      "    │           ├── rust_model.ot\r\n",
      "    │           ├── sentencepiece.bpe.model\r\n",
      "    │           ├── special_tokens_map.json\r\n",
      "    │           ├── tokenizer_config.json\r\n",
      "    │           └── vocab.json\r\n",
      "    ├── \u001b[01;32mbuild_env.sh\u001b[00m\r\n",
      "    ├── config.pbtxt\r\n",
      "    ├── requirements.txt\r\n",
      "    └── \u001b[01;31mtranslationenv.tar.gz\u001b[00m\r\n",
      "\r\n",
      "5 directories, 16 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ./model_repository_gpu_service -I '__pycache__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02fe90ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"translation-service\"\r\n",
      "backend: \"python\"\r\n",
      "max_batch_size: 8\r\n",
      "\r\n",
      "dynamic_batching { }\r\n",
      "\r\n",
      "input [\r\n",
      "  {\r\n",
      "    name: \"TEXT\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims:  [1]\r\n",
      "  },\r\n",
      "  {\r\n",
      "    name: \"SRCLANG\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims:  [1]\r\n",
      "  },\r\n",
      "  {\r\n",
      "    name: \"TARGETLANG\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims:  [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "output [\r\n",
      "  {\r\n",
      "    name: \"TRANSLATION\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims: [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "parameters: {\r\n",
      "  key: \"EXECUTION_ENV_PATH\",\r\n",
      "  value: {string_value: \"/mnt/data/model_repository/translation-service/translationenv.tar.gz\"}\r\n",
      "}\r\n",
      "\r\n",
      "instance_group [\r\n",
      "  {\r\n",
      "    count: 1\r\n",
      "    kind: KIND_GPU\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "response_cache {\r\n",
      "  enable: true\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat model_repository_gpu_service/translation-service/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be3787",
   "metadata": {},
   "source": [
    "### 1.2 Run container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d103d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_id=!(docker run -d \\\n",
    "                --shm-size=5G \\\n",
    "                --gpus device=0 \\\n",
    "                -p8000:8000 -p8001:8001 -p8002:8002 \\\n",
    "                -v $PWD/model_repository_gpu_service:/mnt/data/model_repository \\\n",
    "                nvcr.io/nvidia/tritonserver:23.01-py3 \\\n",
    "                tritonserver \\\n",
    "                --model-repository=/mnt/data/model_repository \\\n",
    "                --log-verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5794329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5493dbf03be993438539ac657e913ac96b906081ea827d4ce6d23a31d9df2599']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4250b405",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "=============================\r\n",
      "== Triton Inference Server ==\r\n",
      "=============================\r\n",
      "\r\n",
      "NVIDIA Release 23.01 (build 52277748)\r\n",
      "Triton Server Version 2.30.0\r\n",
      "\r\n",
      "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\r\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n",
      "\r\n",
      "NOTE: CUDA Forward Compatibility mode ENABLED.\r\n",
      "  Using CUDA 12.0 driver version 525.85.11 with kernel driver version 510.47.03.\r\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\r\n",
      "\r\n",
      "I0226 05:46:55.714760 1 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f37fa000000' with size 268435456\r\n",
      "I0226 05:46:55.717136 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\n",
      "I0226 05:46:55.742747 1 model_config_utils.cc:646] Server side auto-completed config: name: \"translation-service\"\r\n",
      "max_batch_size: 8\r\n",
      "input {\r\n",
      "  name: \"TEXT\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "input {\r\n",
      "  name: \"SRCLANG\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "input {\r\n",
      "  name: \"TARGETLANG\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "output {\r\n",
      "  name: \"TRANSLATION\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "instance_group {\r\n",
      "  count: 1\r\n",
      "  kind: KIND_GPU\r\n",
      "}\r\n",
      "default_model_filename: \"model.py\"\r\n",
      "dynamic_batching {\r\n",
      "}\r\n",
      "parameters {\r\n",
      "  key: \"EXECUTION_ENV_PATH\"\r\n",
      "  value {\r\n",
      "    string_value: \"/mnt/data/model_repository/translation-service/translationenv.tar.gz\"\r\n",
      "  }\r\n",
      "}\r\n",
      "backend: \"python\"\r\n",
      "response_cache {\r\n",
      "  enable: true\r\n",
      "}\r\n",
      "\r\n",
      "W0226 05:46:55.742947 1 model_lifecycle.cc:107] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\r\n",
      "I0226 05:46:55.742976 1 model_lifecycle.cc:459] loading: translation-service:1\r\n",
      "I0226 05:46:55.743129 1 backend_model.cc:348] Adding default backend config setting: default-max-batch-size,4\r\n",
      "I0226 05:46:55.743171 1 shared_library.cc:112] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\r\n",
      "I0226 05:46:55.745289 1 python_be.cc:1614] 'python' TRITONBACKEND API version: 1.11\r\n",
      "I0226 05:46:55.745310 1 python_be.cc:1636] backend configuration:\r\n",
      "{\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\r\n",
      "I0226 05:46:55.745349 1 python_be.cc:1766] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\r\n",
      "I0226 05:46:55.745525 1 python_be.cc:2012] TRITONBACKEND_GetBackendAttribute: setting attributes\r\n",
      "I0226 05:46:56.305446 1 python_be.cc:1814] TRITONBACKEND_ModelInitialize: translation-service (version 1)\r\n",
      "I0226 05:46:56.305963 1 model_config_utils.cc:1838] ModelConfig 64-bit fields:\r\n",
      "I0226 05:46:56.305982 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\r\n",
      "I0226 05:46:56.305987 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\r\n",
      "I0226 05:46:56.305992 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\r\n",
      "I0226 05:46:56.305997 1 model_config_utils.cc:1840] \tModelConfig::ensemble_scheduling::step::model_version\r\n",
      "I0226 05:46:56.306001 1 model_config_utils.cc:1840] \tModelConfig::input::dims\r\n",
      "I0226 05:46:56.306006 1 model_config_utils.cc:1840] \tModelConfig::input::reshape::shape\r\n",
      "I0226 05:46:56.306010 1 model_config_utils.cc:1840] \tModelConfig::instance_group::secondary_devices::device_id\r\n",
      "I0226 05:46:56.306016 1 model_config_utils.cc:1840] \tModelConfig::model_warmup::inputs::value::dims\r\n",
      "I0226 05:46:56.306020 1 model_config_utils.cc:1840] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\r\n",
      "I0226 05:46:56.306025 1 model_config_utils.cc:1840] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\r\n",
      "I0226 05:46:56.306030 1 model_config_utils.cc:1840] \tModelConfig::output::dims\r\n",
      "I0226 05:46:56.306035 1 model_config_utils.cc:1840] \tModelConfig::output::reshape::shape\r\n",
      "I0226 05:46:56.306040 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\r\n",
      "I0226 05:46:56.306045 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\r\n",
      "I0226 05:46:56.306049 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\r\n",
      "I0226 05:46:56.306054 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::state::dims\r\n",
      "I0226 05:46:56.306059 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::state::initial_state::dims\r\n",
      "I0226 05:46:56.306063 1 model_config_utils.cc:1840] \tModelConfig::version_policy::specific::versions\r\n",
      "I0226 05:46:56.306192 1 python_be.cc:1505] Using Python execution env /mnt/data/model_repository/translation-service/translationenv.tar.gz\r\n",
      "I0226 05:48:10.102153 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_q7FV5f/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_q7FV5f/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/translation-service/1/model.py triton_python_backend_shm_region_1 67108864 67108864 1 /opt/tritonserver/backends/python 336 translation-service\r\n",
      "I0226 05:48:12.692423 1 python_be.cc:1594] model configuration:\r\n",
      "{\r\n",
      "    \"name\": \"translation-service\",\r\n",
      "    \"platform\": \"\",\r\n",
      "    \"backend\": \"python\",\r\n",
      "    \"version_policy\": {\r\n",
      "        \"latest\": {\r\n",
      "            \"num_versions\": 1\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"max_batch_size\": 8,\r\n",
      "    \"input\": [\r\n",
      "        {\r\n",
      "            \"name\": \"TEXT\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"format\": \"FORMAT_NONE\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"is_shape_tensor\": false,\r\n",
      "            \"allow_ragged_batch\": false,\r\n",
      "            \"optional\": false\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"SRCLANG\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"format\": \"FORMAT_NONE\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"is_shape_tensor\": false,\r\n",
      "            \"allow_ragged_batch\": false,\r\n",
      "            \"optional\": false\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"TARGETLANG\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"format\": \"FORMAT_NONE\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"is_shape_tensor\": false,\r\n",
      "            \"allow_ragged_batch\": false,\r\n",
      "            \"optional\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"output\": [\r\n",
      "        {\r\n",
      "            \"name\": \"TRANSLATION\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"batch_input\": [],\r\n",
      "    \"batch_output\": [],\r\n",
      "    \"optimization\": {\r\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\r\n",
      "        \"input_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"output_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"gather_kernel_buffer_threshold\": 0,\r\n",
      "        \"eager_batching\": false\r\n",
      "    },\r\n",
      "    \"dynamic_batching\": {\r\n",
      "        \"preferred_batch_size\": [\r\n",
      "            8\r\n",
      "        ],\r\n",
      "        \"max_queue_delay_microseconds\": 0,\r\n",
      "        \"preserve_ordering\": false,\r\n",
      "        \"priority_levels\": 0,\r\n",
      "        \"default_priority_level\": 0,\r\n",
      "        \"priority_queue_policy\": {}\r\n",
      "    },\r\n",
      "    \"instance_group\": [\r\n",
      "        {\r\n",
      "            \"name\": \"translation-service_0\",\r\n",
      "            \"kind\": \"KIND_GPU\",\r\n",
      "            \"count\": 1,\r\n",
      "            \"gpus\": [\r\n",
      "                0\r\n",
      "            ],\r\n",
      "            \"secondary_devices\": [],\r\n",
      "            \"profile\": [],\r\n",
      "            \"passive\": false,\r\n",
      "            \"host_policy\": \"\"\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"default_model_filename\": \"model.py\",\r\n",
      "    \"cc_model_filenames\": {},\r\n",
      "    \"metric_tags\": {},\r\n",
      "    \"parameters\": {\r\n",
      "        \"EXECUTION_ENV_PATH\": {\r\n",
      "            \"string_value\": \"/mnt/data/model_repository/translation-service/translationenv.tar.gz\"\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"model_warmup\": [],\r\n",
      "    \"response_cache\": {\r\n",
      "        \"enable\": true\r\n",
      "    }\r\n",
      "}\r\n",
      "I0226 05:48:12.692628 1 python_be.cc:1858] TRITONBACKEND_ModelInstanceInitialize: translation-service_0 (GPU device 0)\r\n",
      "I0226 05:48:12.693957 1 backend_model_instance.cc:105] Creating instance translation-service_0 on GPU 0 (8.0) using artifact 'model.py'\r\n",
      "I0226 05:48:12.716062 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_q7FV5f/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_q7FV5f/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/translation-service/1/model.py triton_python_backend_shm_region_2 67108864 67108864 1 /opt/tritonserver/backends/python 336 translation-service_0\r\n",
      "I0226 05:48:47.858421 1 python_be.cc:1879] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful translation-service_0 (device 0)\r\n",
      "I0226 05:48:47.858630 1 backend_model_instance.cc:766] Starting backend thread for translation-service_0 at nice 0 on device 0...\r\n",
      "I0226 05:48:47.859007 1 dynamic_batch_scheduler.cc:284] Starting dynamic-batcher thread for translation-service at nice 0...\r\n",
      "I0226 05:48:47.859018 1 model_lifecycle.cc:694] successfully loaded 'translation-service' version 1\r\n",
      "I0226 05:48:47.859149 1 server.cc:563] \r\n",
      "+------------------+------+\r\n",
      "| Repository Agent | Path |\r\n",
      "+------------------+------+\r\n",
      "+------------------+------+\r\n",
      "\r\n",
      "I0226 05:48:47.859216 1 server.cc:590] \r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Backend | Path                                                  | Config                                                                                                                                                        |\r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| python  | /opt/tritonserver/backends/python/libtriton_python.so | {\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0226 05:48:47.859252 1 server.cc:633] \r\n",
      "+---------------------+---------+--------+\r\n",
      "| Model               | Version | Status |\r\n",
      "+---------------------+---------+--------+\r\n",
      "| translation-service | 1       | READY  |\r\n",
      "+---------------------+---------+--------+\r\n",
      "\r\n",
      "I0226 05:48:47.998882 1 metrics.cc:864] Collecting metrics for GPU 0: NVIDIA A100-SXM4-40GB\r\n",
      "I0226 05:48:47.999141 1 metrics.cc:757] Collecting CPU metrics\r\n",
      "I0226 05:48:47.999340 1 tritonserver.cc:2264] \r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Option                           | Value                                                                                                                                                                                                |\r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| server_id                        | triton                                                                                                                                                                                               |\r\n",
      "| server_version                   | 2.30.0                                                                                                                                                                                               |\r\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace logging |\r\n",
      "| model_repository_path[0]         | /mnt/data/model_repository                                                                                                                                                                           |\r\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                            |\r\n",
      "| strict_model_config              | 0                                                                                                                                                                                                    |\r\n",
      "| rate_limit                       | OFF                                                                                                                                                                                                  |\r\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                            |\r\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                             |\r\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                                    |\r\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                                  |\r\n",
      "| strict_readiness                 | 1                                                                                                                                                                                                    |\r\n",
      "| exit_timeout                     | 30                                                                                                                                                                                                   |\r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0226 05:48:47.999978 1 grpc_server.cc:4763] === GRPC KeepAlive Options ===\r\n",
      "I0226 05:48:47.999994 1 grpc_server.cc:4764] keepalive_time_ms: 7200000\r\n",
      "I0226 05:48:48.000000 1 grpc_server.cc:4766] keepalive_timeout_ms: 20000\r\n",
      "I0226 05:48:48.000005 1 grpc_server.cc:4768] keepalive_permit_without_calls: 0\r\n",
      "I0226 05:48:48.000010 1 grpc_server.cc:4770] http2_max_pings_without_data: 2\r\n",
      "I0226 05:48:48.000015 1 grpc_server.cc:4772] http2_min_recv_ping_interval_without_data_ms: 300000\r\n",
      "I0226 05:48:48.000020 1 grpc_server.cc:4775] http2_max_ping_strikes: 2\r\n",
      "I0226 05:48:48.000024 1 grpc_server.cc:4777] ==============================\r\n",
      "I0226 05:48:48.002711 1 grpc_server.cc:225] Ready for RPC 'ServerLive', 0\r\n",
      "I0226 05:48:48.002742 1 grpc_server.cc:225] Ready for RPC 'ServerReady', 0\r\n",
      "I0226 05:48:48.002751 1 grpc_server.cc:225] Ready for RPC 'ModelReady', 0\r\n",
      "I0226 05:48:48.002776 1 grpc_server.cc:225] Ready for RPC 'ServerMetadata', 0\r\n",
      "I0226 05:48:48.002785 1 grpc_server.cc:225] Ready for RPC 'ModelMetadata', 0\r\n",
      "I0226 05:48:48.002810 1 grpc_server.cc:225] Ready for RPC 'ModelConfig', 0\r\n",
      "I0226 05:48:48.002820 1 grpc_server.cc:225] Ready for RPC 'ModelStatistics', 0\r\n",
      "I0226 05:48:48.002833 1 grpc_server.cc:225] Ready for RPC 'Trace', 0\r\n",
      "I0226 05:48:48.002845 1 grpc_server.cc:225] Ready for RPC 'Logging', 0\r\n",
      "I0226 05:48:48.002870 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryStatus', 0\r\n",
      "I0226 05:48:48.002881 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryRegister', 0\r\n",
      "I0226 05:48:48.002890 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryUnregister', 0\r\n",
      "I0226 05:48:48.002915 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryStatus', 0\r\n",
      "I0226 05:48:48.002927 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryRegister', 0\r\n",
      "I0226 05:48:48.002934 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryUnregister', 0\r\n",
      "I0226 05:48:48.002959 1 grpc_server.cc:225] Ready for RPC 'RepositoryIndex', 0\r\n",
      "I0226 05:48:48.002988 1 grpc_server.cc:225] Ready for RPC 'RepositoryModelLoad', 0\r\n",
      "I0226 05:48:48.003000 1 grpc_server.cc:225] Ready for RPC 'RepositoryModelUnload', 0\r\n",
      "I0226 05:48:48.003055 1 grpc_server.cc:419] Thread started for CommonHandler\r\n",
      "I0226 05:48:48.003209 1 grpc_server.cc:3793] New request handler for ModelInferHandler, 0\r\n",
      "I0226 05:48:48.003256 1 grpc_server.cc:2717] Thread started for ModelInferHandler\r\n",
      "I0226 05:48:48.003361 1 grpc_server.cc:3793] New request handler for ModelInferHandler, 0\r\n",
      "I0226 05:48:48.003410 1 grpc_server.cc:2717] Thread started for ModelInferHandler\r\n",
      "I0226 05:48:48.003526 1 grpc_server.cc:4189] New request handler for ModelStreamInferHandler, 0\r\n",
      "I0226 05:48:48.003566 1 grpc_server.cc:2717] Thread started for ModelStreamInferHandler\r\n",
      "I0226 05:48:48.003576 1 grpc_server.cc:4819] Started GRPCInferenceService at 0.0.0.0:8001\r\n",
      "I0226 05:48:48.003841 1 http_server.cc:3477] Started HTTPService at 0.0.0.0:8000\r\n",
      "I0226 05:48:48.044989 1 http_server.cc:184] Started Metrics Service at 0.0.0.0:8002\r\n"
     ]
    }
   ],
   "source": [
    "!docker logs {container_id[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348f347",
   "metadata": {},
   "source": [
    "### 1.3 Inference Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eae81853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'translation-service',\n",
       " 'model_version': '1',\n",
       " 'outputs': [{'name': 'TRANSLATION',\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [1],\n",
       "   'data': ['Thank you for participating in the webinar!']}]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8000/v2/models/translation-service/versions/1/infer\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"inputs\": [\n",
    "    {\n",
    "      \"name\": \"TEXT\",\n",
    "      \"shape\": [\n",
    "        1,\n",
    "        1\n",
    "      ],\n",
    "      \"datatype\": \"BYTES\",\n",
    "      \"data\": [\n",
    "        \"Merci d'avoir participé au webinaire !\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"SRCLANG\",\n",
    "      \"shape\": [\n",
    "        1,\n",
    "        1\n",
    "      ],\n",
    "      \"datatype\": \"BYTES\",\n",
    "      \"data\": [\n",
    "        \"fr\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"TARGETLANG\",\n",
    "      \"shape\": [\n",
    "        1,\n",
    "        1\n",
    "      ],\n",
    "      \"datatype\": \"BYTES\",\n",
    "      \"data\": [\n",
    "        \"en\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\",url , headers=headers, data=payload)\n",
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb960b0",
   "metadata": {},
   "source": [
    "### 2.4 Cleanups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b89f732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5493dbf03be993438539ac657e913ac96b906081ea827d4ce6d23a31d9df2599\r\n"
     ]
    }
   ],
   "source": [
    "!docker stop {container_id[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90585b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! This will remove all stopped containers.\r\n",
      "Are you sure you want to continue? [y/N] Deleted Containers:\r\n",
      "5493dbf03be993438539ac657e913ac96b906081ea827d4ce6d23a31d9df2599\r\n",
      "\r\n",
      "Total reclaimed space: 76.87kB\r\n"
     ]
    }
   ],
   "source": [
    "!echo y | docker container prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53dc38",
   "metadata": {},
   "source": [
    "# III. Optimisation Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86efea8f",
   "metadata": {},
   "source": [
    "## 1. Response Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b290df36",
   "metadata": {},
   "source": [
    "### 1.1 Setups"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bd4effe",
   "metadata": {},
   "source": [
    "Following to be in config.pbtxt\n",
    "\n",
    "response_cache {\n",
    "  enable: true\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84517998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"translation-service\"\r\n",
      "backend: \"python\"\r\n",
      "max_batch_size: 8\r\n",
      "\r\n",
      "dynamic_batching { }\r\n",
      "\r\n",
      "input [\r\n",
      "  {\r\n",
      "    name: \"TEXT\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims:  [1]\r\n",
      "  },\r\n",
      "  {\r\n",
      "    name: \"SRCLANG\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims:  [1]\r\n",
      "  },\r\n",
      "  {\r\n",
      "    name: \"TARGETLANG\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims:  [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "output [\r\n",
      "  {\r\n",
      "    name: \"TRANSLATION\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims: [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "parameters: {\r\n",
      "  key: \"EXECUTION_ENV_PATH\",\r\n",
      "  value: {string_value: \"/mnt/data/model_repository/translation-service/translationenv.tar.gz\"}\r\n",
      "}\r\n",
      "\r\n",
      "instance_group [\r\n",
      "  {\r\n",
      "    count: 1\r\n",
      "    kind: KIND_GPU\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "response_cache {\r\n",
      "  enable: true\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat model_repository_gpu_service/translation-service/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e081a",
   "metadata": {},
   "source": [
    "### 1.2 Run container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc632b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_id=!(docker run -d \\\n",
    "                --shm-size=5G \\\n",
    "                --gpus device=0 \\\n",
    "                -p8000:8000 -p8001:8001 -p8002:8002 \\\n",
    "                -v $PWD/model_repository_gpu_service:/mnt/data/model_repository \\\n",
    "                nvcr.io/nvidia/tritonserver:23.01-py3 \\\n",
    "                tritonserver \\\n",
    "                --model-repository=/mnt/data/model_repository \\\n",
    "                --response-cache-byte-size 1048576 \\\n",
    "                --log-verbose=1)\n",
    "\n",
    "# Note:\n",
    "# --response-cache-byte-size 1048576 is deprecated and changed to --cache-config local,size=SIZE from version 23.03\n",
    "# Support added for redis and custom , addition to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "408d2a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bfc89896cb0c2dc2de4bc185bbb9edecd07a2a6ce945c3e80b20cd25020fc4b9']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0b3eb80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "=============================\r\n",
      "== Triton Inference Server ==\r\n",
      "=============================\r\n",
      "\r\n",
      "NVIDIA Release 23.01 (build 52277748)\r\n",
      "Triton Server Version 2.30.0\r\n",
      "\r\n",
      "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\r\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n",
      "\r\n",
      "NOTE: CUDA Forward Compatibility mode ENABLED.\r\n",
      "  Using CUDA 12.0 driver version 525.85.11 with kernel driver version 510.47.03.\r\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\r\n",
      "\r\n",
      "I0226 05:52:11.893818 1 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f374e000000' with size 268435456\r\n",
      "I0226 05:52:11.893995 1 response_cache.cc:115] Response Cache is created at '0x563ae1c61b50' with size 1048576\r\n",
      "I0226 05:52:11.896154 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\r\n",
      "I0226 05:52:11.900769 1 model_config_utils.cc:646] Server side auto-completed config: name: \"translation-service\"\r\n",
      "max_batch_size: 8\r\n",
      "input {\r\n",
      "  name: \"TEXT\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "input {\r\n",
      "  name: \"SRCLANG\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "input {\r\n",
      "  name: \"TARGETLANG\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "output {\r\n",
      "  name: \"TRANSLATION\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "instance_group {\r\n",
      "  count: 1\r\n",
      "  kind: KIND_GPU\r\n",
      "}\r\n",
      "default_model_filename: \"model.py\"\r\n",
      "dynamic_batching {\r\n",
      "}\r\n",
      "parameters {\r\n",
      "  key: \"EXECUTION_ENV_PATH\"\r\n",
      "  value {\r\n",
      "    string_value: \"/mnt/data/model_repository/translation-service/translationenv.tar.gz\"\r\n",
      "  }\r\n",
      "}\r\n",
      "backend: \"python\"\r\n",
      "response_cache {\r\n",
      "  enable: true\r\n",
      "}\r\n",
      "\r\n",
      "W0226 05:52:11.900976 1 model_lifecycle.cc:107] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\r\n",
      "I0226 05:52:11.901016 1 model_lifecycle.cc:459] loading: translation-service:1\r\n",
      "I0226 05:52:11.901203 1 backend_model.cc:348] Adding default backend config setting: default-max-batch-size,4\r\n",
      "I0226 05:52:11.901245 1 shared_library.cc:112] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\r\n",
      "I0226 05:52:11.903342 1 python_be.cc:1614] 'python' TRITONBACKEND API version: 1.11\r\n",
      "I0226 05:52:11.903365 1 python_be.cc:1636] backend configuration:\r\n",
      "{\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\r\n",
      "I0226 05:52:11.903404 1 python_be.cc:1766] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\r\n",
      "I0226 05:52:11.903568 1 python_be.cc:2012] TRITONBACKEND_GetBackendAttribute: setting attributes\r\n",
      "I0226 05:52:11.960061 1 python_be.cc:1814] TRITONBACKEND_ModelInitialize: translation-service (version 1)\r\n",
      "I0226 05:52:11.960567 1 model_config_utils.cc:1838] ModelConfig 64-bit fields:\r\n",
      "I0226 05:52:11.960585 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\r\n",
      "I0226 05:52:11.960590 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\r\n",
      "I0226 05:52:11.960595 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\r\n",
      "I0226 05:52:11.960600 1 model_config_utils.cc:1840] \tModelConfig::ensemble_scheduling::step::model_version\r\n",
      "I0226 05:52:11.960606 1 model_config_utils.cc:1840] \tModelConfig::input::dims\r\n",
      "I0226 05:52:11.960610 1 model_config_utils.cc:1840] \tModelConfig::input::reshape::shape\r\n",
      "I0226 05:52:11.960616 1 model_config_utils.cc:1840] \tModelConfig::instance_group::secondary_devices::device_id\r\n",
      "I0226 05:52:11.960621 1 model_config_utils.cc:1840] \tModelConfig::model_warmup::inputs::value::dims\r\n",
      "I0226 05:52:11.960626 1 model_config_utils.cc:1840] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\r\n",
      "I0226 05:52:11.960631 1 model_config_utils.cc:1840] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\r\n",
      "I0226 05:52:11.960637 1 model_config_utils.cc:1840] \tModelConfig::output::dims\r\n",
      "I0226 05:52:11.960642 1 model_config_utils.cc:1840] \tModelConfig::output::reshape::shape\r\n",
      "I0226 05:52:11.960647 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\r\n",
      "I0226 05:52:11.960652 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\r\n",
      "I0226 05:52:11.960657 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\r\n",
      "I0226 05:52:11.960663 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::state::dims\r\n",
      "I0226 05:52:11.960668 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::state::initial_state::dims\r\n",
      "I0226 05:52:11.960673 1 model_config_utils.cc:1840] \tModelConfig::version_policy::specific::versions\r\n",
      "I0226 05:52:11.960803 1 python_be.cc:1505] Using Python execution env /mnt/data/model_repository/translation-service/translationenv.tar.gz\r\n",
      "I0226 05:52:41.519661 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_j9glmg/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_j9glmg/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/translation-service/1/model.py triton_python_backend_shm_region_1 67108864 67108864 1 /opt/tritonserver/backends/python 336 translation-service\r\n",
      "I0226 05:52:44.129717 1 python_be.cc:1594] model configuration:\r\n",
      "{\r\n",
      "    \"name\": \"translation-service\",\r\n",
      "    \"platform\": \"\",\r\n",
      "    \"backend\": \"python\",\r\n",
      "    \"version_policy\": {\r\n",
      "        \"latest\": {\r\n",
      "            \"num_versions\": 1\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"max_batch_size\": 8,\r\n",
      "    \"input\": [\r\n",
      "        {\r\n",
      "            \"name\": \"TEXT\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"format\": \"FORMAT_NONE\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"is_shape_tensor\": false,\r\n",
      "            \"allow_ragged_batch\": false,\r\n",
      "            \"optional\": false\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"SRCLANG\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"format\": \"FORMAT_NONE\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"is_shape_tensor\": false,\r\n",
      "            \"allow_ragged_batch\": false,\r\n",
      "            \"optional\": false\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"TARGETLANG\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"format\": \"FORMAT_NONE\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"is_shape_tensor\": false,\r\n",
      "            \"allow_ragged_batch\": false,\r\n",
      "            \"optional\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"output\": [\r\n",
      "        {\r\n",
      "            \"name\": \"TRANSLATION\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"batch_input\": [],\r\n",
      "    \"batch_output\": [],\r\n",
      "    \"optimization\": {\r\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\r\n",
      "        \"input_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"output_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"gather_kernel_buffer_threshold\": 0,\r\n",
      "        \"eager_batching\": false\r\n",
      "    },\r\n",
      "    \"dynamic_batching\": {\r\n",
      "        \"preferred_batch_size\": [\r\n",
      "            8\r\n",
      "        ],\r\n",
      "        \"max_queue_delay_microseconds\": 0,\r\n",
      "        \"preserve_ordering\": false,\r\n",
      "        \"priority_levels\": 0,\r\n",
      "        \"default_priority_level\": 0,\r\n",
      "        \"priority_queue_policy\": {}\r\n",
      "    },\r\n",
      "    \"instance_group\": [\r\n",
      "        {\r\n",
      "            \"name\": \"translation-service_0\",\r\n",
      "            \"kind\": \"KIND_GPU\",\r\n",
      "            \"count\": 1,\r\n",
      "            \"gpus\": [\r\n",
      "                0\r\n",
      "            ],\r\n",
      "            \"secondary_devices\": [],\r\n",
      "            \"profile\": [],\r\n",
      "            \"passive\": false,\r\n",
      "            \"host_policy\": \"\"\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"default_model_filename\": \"model.py\",\r\n",
      "    \"cc_model_filenames\": {},\r\n",
      "    \"metric_tags\": {},\r\n",
      "    \"parameters\": {\r\n",
      "        \"EXECUTION_ENV_PATH\": {\r\n",
      "            \"string_value\": \"/mnt/data/model_repository/translation-service/translationenv.tar.gz\"\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"model_warmup\": [],\r\n",
      "    \"response_cache\": {\r\n",
      "        \"enable\": true\r\n",
      "    }\r\n",
      "}\r\n",
      "I0226 05:52:44.129950 1 python_be.cc:1858] TRITONBACKEND_ModelInstanceInitialize: translation-service_0 (GPU device 0)\r\n",
      "I0226 05:52:44.131422 1 backend_model_instance.cc:105] Creating instance translation-service_0 on GPU 0 (8.0) using artifact 'model.py'\r\n",
      "I0226 05:52:44.153935 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_j9glmg/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_j9glmg/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/translation-service/1/model.py triton_python_backend_shm_region_2 67108864 67108864 1 /opt/tritonserver/backends/python 336 translation-service_0\r\n",
      "I0226 05:52:57.150413 1 python_be.cc:1879] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful translation-service_0 (device 0)\r\n",
      "I0226 05:52:57.150628 1 backend_model_instance.cc:766] Starting backend thread for translation-service_0 at nice 0 on device 0...\r\n",
      "I0226 05:52:57.151061 1 model_lifecycle.cc:694] successfully loaded 'translation-service' version 1\r\n",
      "I0226 05:52:57.151078 1 dynamic_batch_scheduler.cc:284] Starting dynamic-batcher thread for translation-service at nice 0...\r\n",
      "I0226 05:52:57.151209 1 server.cc:563] \r\n",
      "+------------------+------+\r\n",
      "| Repository Agent | Path |\r\n",
      "+------------------+------+\r\n",
      "+------------------+------+\r\n",
      "\r\n",
      "I0226 05:52:57.151283 1 server.cc:590] \r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Backend | Path                                                  | Config                                                                                                                                                        |\r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| python  | /opt/tritonserver/backends/python/libtriton_python.so | {\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0226 05:52:57.151330 1 server.cc:633] \r\n",
      "+---------------------+---------+--------+\r\n",
      "| Model               | Version | Status |\r\n",
      "+---------------------+---------+--------+\r\n",
      "| translation-service | 1       | READY  |\r\n",
      "+---------------------+---------+--------+\r\n",
      "\r\n",
      "I0226 05:52:57.151352 1 metrics.cc:722] Collecting Response Cache metrics\r\n",
      "I0226 05:52:57.175335 1 metrics.cc:864] Collecting metrics for GPU 0: NVIDIA A100-SXM4-40GB\r\n",
      "I0226 05:52:57.175627 1 metrics.cc:757] Collecting CPU metrics\r\n",
      "I0226 05:52:57.175855 1 tritonserver.cc:2264] \r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Option                           | Value                                                                                                                                                                                                |\r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| server_id                        | triton                                                                                                                                                                                               |\r\n",
      "| server_version                   | 2.30.0                                                                                                                                                                                               |\r\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace logging |\r\n",
      "| model_repository_path[0]         | /mnt/data/model_repository                                                                                                                                                                           |\r\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                            |\r\n",
      "| strict_model_config              | 0                                                                                                                                                                                                    |\r\n",
      "| rate_limit                       | OFF                                                                                                                                                                                                  |\r\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                            |\r\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                             |\r\n",
      "| response_cache_byte_size         | 1048576                                                                                                                                                                                              |\r\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                                  |\r\n",
      "| strict_readiness                 | 1                                                                                                                                                                                                    |\r\n",
      "| exit_timeout                     | 30                                                                                                                                                                                                   |\r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0226 05:52:57.176578 1 grpc_server.cc:4763] === GRPC KeepAlive Options ===\r\n",
      "I0226 05:52:57.176597 1 grpc_server.cc:4764] keepalive_time_ms: 7200000\r\n",
      "I0226 05:52:57.176602 1 grpc_server.cc:4766] keepalive_timeout_ms: 20000\r\n",
      "I0226 05:52:57.176607 1 grpc_server.cc:4768] keepalive_permit_without_calls: 0\r\n",
      "I0226 05:52:57.176612 1 grpc_server.cc:4770] http2_max_pings_without_data: 2\r\n",
      "I0226 05:52:57.176617 1 grpc_server.cc:4772] http2_min_recv_ping_interval_without_data_ms: 300000\r\n",
      "I0226 05:52:57.176622 1 grpc_server.cc:4775] http2_max_ping_strikes: 2\r\n",
      "I0226 05:52:57.176627 1 grpc_server.cc:4777] ==============================\r\n",
      "I0226 05:52:57.179388 1 grpc_server.cc:225] Ready for RPC 'ServerLive', 0\r\n",
      "I0226 05:52:57.179417 1 grpc_server.cc:225] Ready for RPC 'ServerReady', 0\r\n",
      "I0226 05:52:57.179426 1 grpc_server.cc:225] Ready for RPC 'ModelReady', 0\r\n",
      "I0226 05:52:57.179458 1 grpc_server.cc:225] Ready for RPC 'ServerMetadata', 0\r\n",
      "I0226 05:52:57.179477 1 grpc_server.cc:225] Ready for RPC 'ModelMetadata', 0\r\n",
      "I0226 05:52:57.179516 1 grpc_server.cc:225] Ready for RPC 'ModelConfig', 0\r\n",
      "I0226 05:52:57.179541 1 grpc_server.cc:225] Ready for RPC 'ModelStatistics', 0\r\n",
      "I0226 05:52:57.179562 1 grpc_server.cc:225] Ready for RPC 'Trace', 0\r\n",
      "I0226 05:52:57.179579 1 grpc_server.cc:225] Ready for RPC 'Logging', 0\r\n",
      "I0226 05:52:57.179613 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryStatus', 0\r\n",
      "I0226 05:52:57.179632 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryRegister', 0\r\n",
      "I0226 05:52:57.179647 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryUnregister', 0\r\n",
      "I0226 05:52:57.179665 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryStatus', 0\r\n",
      "I0226 05:52:57.179680 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryRegister', 0\r\n",
      "I0226 05:52:57.179695 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryUnregister', 0\r\n",
      "I0226 05:52:57.179738 1 grpc_server.cc:225] Ready for RPC 'RepositoryIndex', 0\r\n",
      "I0226 05:52:57.179774 1 grpc_server.cc:225] Ready for RPC 'RepositoryModelLoad', 0\r\n",
      "I0226 05:52:57.179788 1 grpc_server.cc:225] Ready for RPC 'RepositoryModelUnload', 0\r\n",
      "I0226 05:52:57.179831 1 grpc_server.cc:419] Thread started for CommonHandler\r\n",
      "I0226 05:52:57.179957 1 grpc_server.cc:3793] New request handler for ModelInferHandler, 0\r\n",
      "I0226 05:52:57.180010 1 grpc_server.cc:2717] Thread started for ModelInferHandler\r\n",
      "I0226 05:52:57.180155 1 grpc_server.cc:3793] New request handler for ModelInferHandler, 0\r\n",
      "I0226 05:52:57.180210 1 grpc_server.cc:2717] Thread started for ModelInferHandler\r\n",
      "I0226 05:52:57.180342 1 grpc_server.cc:4189] New request handler for ModelStreamInferHandler, 0\r\n",
      "I0226 05:52:57.180415 1 grpc_server.cc:2717] Thread started for ModelStreamInferHandler\r\n",
      "I0226 05:52:57.180433 1 grpc_server.cc:4819] Started GRPCInferenceService at 0.0.0.0:8001\r\n",
      "I0226 05:52:57.180724 1 http_server.cc:3477] Started HTTPService at 0.0.0.0:8000\r\n",
      "I0226 05:52:57.222160 1 http_server.cc:184] Started Metrics Service at 0.0.0.0:8002\r\n"
     ]
    }
   ],
   "source": [
    "!docker logs {container_id[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33495166",
   "metadata": {},
   "source": [
    "### 1.3 Inference Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31d0fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8000/v2/models/translation-service/versions/1/infer\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"inputs\": [\n",
    "    {\n",
    "      \"name\": \"TEXT\",\n",
    "      \"shape\": [\n",
    "        1,\n",
    "        1\n",
    "      ],\n",
    "      \"datatype\": \"BYTES\",\n",
    "      \"data\": [\n",
    "        \"Merci d'avoir participé au webinaire !\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"SRCLANG\",\n",
    "      \"shape\": [\n",
    "        1,\n",
    "        1\n",
    "      ],\n",
    "      \"datatype\": \"BYTES\",\n",
    "      \"data\": [\n",
    "        \"fr\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"TARGETLANG\",\n",
    "      \"shape\": [\n",
    "        1,\n",
    "        1\n",
    "      ],\n",
    "      \"datatype\": \"BYTES\",\n",
    "      \"data\": [\n",
    "        \"en\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbd1bb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'translation-service',\n",
       " 'model_version': '1',\n",
       " 'outputs': [{'name': 'TRANSLATION',\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [1],\n",
       "   'data': ['Thank you for participating in the webinar!']}]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First call\n",
    "response = requests.request(\"POST\",url , headers=headers, data=payload)\n",
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27d890bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.319397"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.elapsed.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7af7e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'translation-service',\n",
       " 'model_version': '1',\n",
       " 'outputs': [{'name': 'TRANSLATION',\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [1],\n",
       "   'data': ['Thank you for participating in the webinar!']}]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second call with same input - To be fetched from cache\n",
    "response = requests.request(\"POST\",url , headers=headers, data=payload)\n",
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0f1a470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002314"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.elapsed.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce9daa",
   "metadata": {},
   "source": [
    "### 2.4 Cleanups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bffacf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfc89896cb0c2dc2de4bc185bbb9edecd07a2a6ce945c3e80b20cd25020fc4b9\r\n"
     ]
    }
   ],
   "source": [
    "!docker stop {container_id[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4cf00ac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! This will remove all stopped containers.\n",
      "Are you sure you want to continue? [y/N] Deleted Containers:\n",
      "bfc89896cb0c2dc2de4bc185bbb9edecd07a2a6ce945c3e80b20cd25020fc4b9\n",
      "\n",
      "Total reclaimed space: 76.38kB\n"
     ]
    }
   ],
   "source": [
    "!echo y | docker container prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efcb6c8",
   "metadata": {},
   "source": [
    "## 2. Instance groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421488a",
   "metadata": {},
   "source": [
    "### 1.1 Setups"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb7c492e",
   "metadata": {},
   "source": [
    "Update below section config.pbtxt\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 4\n",
    "    kind: KIND_CPU\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "153d7430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"sentiment-nltk-service-multi-instance\"\r\n",
      "backend: \"python\"\r\n",
      "max_batch_size: 1\r\n",
      "\r\n",
      "dynamic_batching { }\r\n",
      "\r\n",
      "input [\r\n",
      "  {\r\n",
      "    name: \"TEXT\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims:  [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "output [\r\n",
      "  {\r\n",
      "    name: \"STATUS\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims: [1]\r\n",
      "  },\r\n",
      "  {\r\n",
      "    name: \"SCORE\"\r\n",
      "    data_type: TYPE_FP32\r\n",
      "    dims: [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "parameters: {\r\n",
      "  key: \"EXECUTION_ENV_PATH\",\r\n",
      "  value: {string_value: \"/mnt/data/model_repository/sentiment-nltk-service-multi-instance/sentinltkenv.tar.gz\"}\r\n",
      "}\r\n",
      "\r\n",
      "instance_group [\r\n",
      "  {\r\n",
      "    count: 4\r\n",
      "    kind: KIND_CPU\r\n",
      "  }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!cat model_repository_instance_group/sentiment-nltk-service-multi-instance/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4143b4",
   "metadata": {},
   "source": [
    "### 1.2 Run container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9ecf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_id=!(docker run -d \\\n",
    "                --shm-size=5G \\\n",
    "                -p8000:8000 -p8001:8001 -p8002:8002 \\\n",
    "                -v $PWD/model_repository_instance_group:/mnt/data/model_repository \\\n",
    "                nvcr.io/nvidia/tritonserver:23.01-py3 \\\n",
    "                tritonserver \\\n",
    "                --model-repository=/mnt/data/model_repository \\\n",
    "                --response-cache-byte-size 1048576 \\\n",
    "                --log-verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c5cc261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['07be1398dba9332f55919abb0d9f1d75d1c5fc1d4c13fdcd0aca8781ff28ac5a']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df4c05c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "=============================\r\n",
      "== Triton Inference Server ==\r\n",
      "=============================\r\n",
      "\r\n",
      "NVIDIA Release 23.01 (build 52277748)\r\n",
      "Triton Server Version 2.30.0\r\n",
      "\r\n",
      "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\r\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n",
      "\r\n",
      "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n",
      "   Use the NVIDIA Container Toolkit to start this container with GPU support; see\r\n",
      "   https://docs.nvidia.com/datacenter/cloud-native/ .\r\n",
      "\r\n",
      "W0226 05:55:14.933129 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\r\n",
      "I0226 05:55:14.933282 1 response_cache.cc:115] Response Cache is created at '0x7fd5e2c21010' with size 1048576\r\n",
      "I0226 05:55:14.933329 1 cuda_memory_manager.cc:115] CUDA memory pool disabled\r\n",
      "I0226 05:55:14.947623 1 model_config_utils.cc:646] Server side auto-completed config: name: \"sentiment-nltk-service\"\r\n",
      "max_batch_size: 1\r\n",
      "input {\r\n",
      "  name: \"TEXT\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "output {\r\n",
      "  name: \"STATUS\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "output {\r\n",
      "  name: \"SCORE\"\r\n",
      "  data_type: TYPE_FP32\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "instance_group {\r\n",
      "  count: 1\r\n",
      "  kind: KIND_CPU\r\n",
      "}\r\n",
      "default_model_filename: \"model.py\"\r\n",
      "dynamic_batching {\r\n",
      "}\r\n",
      "parameters {\r\n",
      "  key: \"EXECUTION_ENV_PATH\"\r\n",
      "  value {\r\n",
      "    string_value: \"/mnt/data/model_repository/sentiment-nltk-service/sentinltkenv.tar.gz\"\r\n",
      "  }\r\n",
      "}\r\n",
      "backend: \"python\"\r\n",
      "\r\n",
      "I0226 05:55:14.947986 1 model_config_utils.cc:646] Server side auto-completed config: name: \"sentiment-nltk-service-multi-instance\"\r\n",
      "max_batch_size: 1\r\n",
      "input {\r\n",
      "  name: \"TEXT\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "output {\r\n",
      "  name: \"STATUS\"\r\n",
      "  data_type: TYPE_STRING\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "output {\r\n",
      "  name: \"SCORE\"\r\n",
      "  data_type: TYPE_FP32\r\n",
      "  dims: 1\r\n",
      "}\r\n",
      "instance_group {\r\n",
      "  count: 4\r\n",
      "  kind: KIND_CPU\r\n",
      "}\r\n",
      "default_model_filename: \"model.py\"\r\n",
      "dynamic_batching {\r\n",
      "}\r\n",
      "parameters {\r\n",
      "  key: \"EXECUTION_ENV_PATH\"\r\n",
      "  value {\r\n",
      "    string_value: \"/mnt/data/model_repository/sentiment-nltk-service-multi-instance/sentinltkenv.tar.gz\"\r\n",
      "  }\r\n",
      "}\r\n",
      "backend: \"python\"\r\n",
      "\r\n",
      "W0226 05:55:14.948174 1 model_lifecycle.cc:107] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\r\n",
      "I0226 05:55:14.948211 1 model_lifecycle.cc:459] loading: sentiment-nltk-service-multi-instance:1\r\n",
      "W0226 05:55:14.948290 1 model_lifecycle.cc:107] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\r\n",
      "I0226 05:55:14.948310 1 model_lifecycle.cc:459] loading: sentiment-nltk-service:1\r\n",
      "I0226 05:55:14.948344 1 backend_model.cc:348] Adding default backend config setting: default-max-batch-size,4\r\n",
      "I0226 05:55:14.948384 1 shared_library.cc:112] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\r\n",
      "I0226 05:55:14.948453 1 backend_model.cc:348] Adding default backend config setting: default-max-batch-size,4\r\n",
      "I0226 05:55:14.950237 1 python_be.cc:1614] 'python' TRITONBACKEND API version: 1.11\r\n",
      "I0226 05:55:14.950258 1 python_be.cc:1636] backend configuration:\r\n",
      "{\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\r\n",
      "I0226 05:55:14.950297 1 python_be.cc:1766] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\r\n",
      "I0226 05:55:14.950456 1 python_be.cc:2012] TRITONBACKEND_GetBackendAttribute: setting attributes\r\n",
      "I0226 05:55:14.950492 1 python_be.cc:1814] TRITONBACKEND_ModelInitialize: sentiment-nltk-service-multi-instance (version 1)\r\n",
      "I0226 05:55:14.950967 1 model_config_utils.cc:1838] ModelConfig 64-bit fields:\r\n",
      "I0226 05:55:14.950985 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\r\n",
      "I0226 05:55:14.950991 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\r\n",
      "I0226 05:55:14.950996 1 model_config_utils.cc:1840] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\r\n",
      "I0226 05:55:14.951001 1 model_config_utils.cc:1840] \tModelConfig::ensemble_scheduling::step::model_version\r\n",
      "I0226 05:55:14.951006 1 model_config_utils.cc:1840] \tModelConfig::input::dims\r\n",
      "I0226 05:55:14.951025 1 model_config_utils.cc:1840] \tModelConfig::input::reshape::shape\r\n",
      "I0226 05:55:14.951030 1 model_config_utils.cc:1840] \tModelConfig::instance_group::secondary_devices::device_id\r\n",
      "I0226 05:55:14.951034 1 model_config_utils.cc:1840] \tModelConfig::model_warmup::inputs::value::dims\r\n",
      "I0226 05:55:14.951039 1 model_config_utils.cc:1840] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\r\n",
      "I0226 05:55:14.951044 1 model_config_utils.cc:1840] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\r\n",
      "I0226 05:55:14.951049 1 model_config_utils.cc:1840] \tModelConfig::output::dims\r\n",
      "I0226 05:55:14.951054 1 model_config_utils.cc:1840] \tModelConfig::output::reshape::shape\r\n",
      "I0226 05:55:14.951059 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\r\n",
      "I0226 05:55:14.951065 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\r\n",
      "I0226 05:55:14.951070 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\r\n",
      "I0226 05:55:14.951075 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::state::dims\r\n",
      "I0226 05:55:14.951079 1 model_config_utils.cc:1840] \tModelConfig::sequence_batching::state::initial_state::dims\r\n",
      "I0226 05:55:14.951084 1 model_config_utils.cc:1840] \tModelConfig::version_policy::specific::versions\r\n",
      "I0226 05:55:14.951213 1 python_be.cc:1505] Using Python execution env /mnt/data/model_repository/sentiment-nltk-service-multi-instance/sentinltkenv.tar.gz\r\n",
      "I0226 05:55:17.078129 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_4RgPui/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_4RgPui/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-nltk-service-multi-instance/1/model.py triton_python_backend_shm_region_1 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-nltk-service-multi-instance\r\n",
      "I0226 05:55:18.676241 1 python_be.cc:1594] model configuration:\r\n",
      "{\r\n",
      "    \"name\": \"sentiment-nltk-service-multi-instance\",\r\n",
      "    \"platform\": \"\",\r\n",
      "    \"backend\": \"python\",\r\n",
      "    \"version_policy\": {\r\n",
      "        \"latest\": {\r\n",
      "            \"num_versions\": 1\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"max_batch_size\": 1,\r\n",
      "    \"input\": [\r\n",
      "        {\r\n",
      "            \"name\": \"TEXT\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"format\": \"FORMAT_NONE\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"is_shape_tensor\": false,\r\n",
      "            \"allow_ragged_batch\": false,\r\n",
      "            \"optional\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"output\": [\r\n",
      "        {\r\n",
      "            \"name\": \"STATUS\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"SCORE\",\r\n",
      "            \"data_type\": \"TYPE_FP32\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"batch_input\": [],\r\n",
      "    \"batch_output\": [],\r\n",
      "    \"optimization\": {\r\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\r\n",
      "        \"input_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"output_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"gather_kernel_buffer_threshold\": 0,\r\n",
      "        \"eager_batching\": false\r\n",
      "    },\r\n",
      "    \"dynamic_batching\": {\r\n",
      "        \"preferred_batch_size\": [\r\n",
      "            1\r\n",
      "        ],\r\n",
      "        \"max_queue_delay_microseconds\": 0,\r\n",
      "        \"preserve_ordering\": false,\r\n",
      "        \"priority_levels\": 0,\r\n",
      "        \"default_priority_level\": 0,\r\n",
      "        \"priority_queue_policy\": {}\r\n",
      "    },\r\n",
      "    \"instance_group\": [\r\n",
      "        {\r\n",
      "            \"name\": \"sentiment-nltk-service-multi-instance_0\",\r\n",
      "            \"kind\": \"KIND_CPU\",\r\n",
      "            \"count\": 4,\r\n",
      "            \"gpus\": [],\r\n",
      "            \"secondary_devices\": [],\r\n",
      "            \"profile\": [],\r\n",
      "            \"passive\": false,\r\n",
      "            \"host_policy\": \"\"\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"default_model_filename\": \"model.py\",\r\n",
      "    \"cc_model_filenames\": {},\r\n",
      "    \"metric_tags\": {},\r\n",
      "    \"parameters\": {\r\n",
      "        \"EXECUTION_ENV_PATH\": {\r\n",
      "            \"string_value\": \"/mnt/data/model_repository/sentiment-nltk-service-multi-instance/sentinltkenv.tar.gz\"\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"model_warmup\": []\r\n",
      "}\r\n",
      "I0226 05:55:18.676349 1 python_be.cc:1814] TRITONBACKEND_ModelInitialize: sentiment-nltk-service (version 1)\r\n",
      "I0226 05:55:18.676871 1 python_be.cc:1505] Using Python execution env /mnt/data/model_repository/sentiment-nltk-service/sentinltkenv.tar.gz\r\n",
      "I0226 05:55:20.641925 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_4RgPui/1/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_4RgPui/1/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-nltk-service/1/model.py triton_python_backend_shm_region_2 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-nltk-service\r\n",
      "I0226 05:55:22.239478 1 python_be.cc:1594] model configuration:\r\n",
      "{\r\n",
      "    \"name\": \"sentiment-nltk-service\",\r\n",
      "    \"platform\": \"\",\r\n",
      "    \"backend\": \"python\",\r\n",
      "    \"version_policy\": {\r\n",
      "        \"latest\": {\r\n",
      "            \"num_versions\": 1\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"max_batch_size\": 1,\r\n",
      "    \"input\": [\r\n",
      "        {\r\n",
      "            \"name\": \"TEXT\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"format\": \"FORMAT_NONE\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"is_shape_tensor\": false,\r\n",
      "            \"allow_ragged_batch\": false,\r\n",
      "            \"optional\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"output\": [\r\n",
      "        {\r\n",
      "            \"name\": \"STATUS\",\r\n",
      "            \"data_type\": \"TYPE_STRING\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"SCORE\",\r\n",
      "            \"data_type\": \"TYPE_FP32\",\r\n",
      "            \"dims\": [\r\n",
      "                1\r\n",
      "            ],\r\n",
      "            \"label_filename\": \"\",\r\n",
      "            \"is_shape_tensor\": false\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"batch_input\": [],\r\n",
      "    \"batch_output\": [],\r\n",
      "    \"optimization\": {\r\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\r\n",
      "        \"input_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"output_pinned_memory\": {\r\n",
      "            \"enable\": true\r\n",
      "        },\r\n",
      "        \"gather_kernel_buffer_threshold\": 0,\r\n",
      "        \"eager_batching\": false\r\n",
      "    },\r\n",
      "    \"dynamic_batching\": {\r\n",
      "        \"preferred_batch_size\": [\r\n",
      "            1\r\n",
      "        ],\r\n",
      "        \"max_queue_delay_microseconds\": 0,\r\n",
      "        \"preserve_ordering\": false,\r\n",
      "        \"priority_levels\": 0,\r\n",
      "        \"default_priority_level\": 0,\r\n",
      "        \"priority_queue_policy\": {}\r\n",
      "    },\r\n",
      "    \"instance_group\": [\r\n",
      "        {\r\n",
      "            \"name\": \"sentiment-nltk-service_0\",\r\n",
      "            \"kind\": \"KIND_CPU\",\r\n",
      "            \"count\": 1,\r\n",
      "            \"gpus\": [],\r\n",
      "            \"secondary_devices\": [],\r\n",
      "            \"profile\": [],\r\n",
      "            \"passive\": false,\r\n",
      "            \"host_policy\": \"\"\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"default_model_filename\": \"model.py\",\r\n",
      "    \"cc_model_filenames\": {},\r\n",
      "    \"metric_tags\": {},\r\n",
      "    \"parameters\": {\r\n",
      "        \"EXECUTION_ENV_PATH\": {\r\n",
      "            \"string_value\": \"/mnt/data/model_repository/sentiment-nltk-service/sentinltkenv.tar.gz\"\r\n",
      "        }\r\n",
      "    },\r\n",
      "    \"model_warmup\": []\r\n",
      "}\r\n",
      "I0226 05:55:22.239644 1 python_be.cc:1858] TRITONBACKEND_ModelInstanceInitialize: sentiment-nltk-service-multi-instance_0_0 (CPU device 0)\r\n",
      "I0226 05:55:22.239697 1 backend_model_instance.cc:68] Creating instance sentiment-nltk-service-multi-instance_0_0 on CPU using artifact 'model.py'\r\n",
      "I0226 05:55:22.261618 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_4RgPui/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_4RgPui/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-nltk-service-multi-instance/1/model.py triton_python_backend_shm_region_3 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-nltk-service-multi-instance_0_0\r\n",
      "I0226 05:55:22.744757 1 python_be.cc:1879] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful sentiment-nltk-service-multi-instance_0_0 (device 0)\r\n",
      "I0226 05:55:22.744848 1 python_be.cc:1858] TRITONBACKEND_ModelInstanceInitialize: sentiment-nltk-service_0 (CPU device 0)\r\n",
      "I0226 05:55:22.744879 1 backend_model_instance.cc:68] Creating instance sentiment-nltk-service_0 on CPU using artifact 'model.py'\r\n",
      "I0226 05:55:22.744917 1 backend_model_instance.cc:766] Starting backend thread for sentiment-nltk-service-multi-instance_0_0 at nice 0 on device 0...\r\n",
      "I0226 05:55:22.767065 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_4RgPui/1/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_4RgPui/1/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-nltk-service/1/model.py triton_python_backend_shm_region_4 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-nltk-service_0\r\n",
      "I0226 05:55:23.176134 1 python_be.cc:1879] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful sentiment-nltk-service_0 (device 0)\r\n",
      "I0226 05:55:23.176253 1 python_be.cc:1858] TRITONBACKEND_ModelInstanceInitialize: sentiment-nltk-service-multi-instance_0_1 (CPU device 0)\r\n",
      "I0226 05:55:23.176268 1 backend_model_instance.cc:766] Starting backend thread for sentiment-nltk-service_0 at nice 0 on device 0...\r\n",
      "I0226 05:55:23.176298 1 backend_model_instance.cc:68] Creating instance sentiment-nltk-service-multi-instance_0_1 on CPU using artifact 'model.py'\r\n",
      "I0226 05:55:23.176558 1 model_lifecycle.cc:694] successfully loaded 'sentiment-nltk-service' version 1\r\n",
      "I0226 05:55:23.176578 1 dynamic_batch_scheduler.cc:284] Starting dynamic-batcher thread for sentiment-nltk-service at nice 0...\r\n",
      "I0226 05:55:23.198573 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_4RgPui/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_4RgPui/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-nltk-service-multi-instance/1/model.py triton_python_backend_shm_region_5 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-nltk-service-multi-instance_0_1\r\n",
      "I0226 05:55:23.611417 1 python_be.cc:1879] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful sentiment-nltk-service-multi-instance_0_1 (device 0)\r\n",
      "I0226 05:55:23.611571 1 backend_model_instance.cc:766] Starting backend thread for sentiment-nltk-service-multi-instance_0_1 at nice 0 on device 0...\r\n",
      "I0226 05:55:23.611711 1 python_be.cc:1858] TRITONBACKEND_ModelInstanceInitialize: sentiment-nltk-service-multi-instance_0_2 (CPU device 0)\r\n",
      "I0226 05:55:23.611732 1 backend_model_instance.cc:68] Creating instance sentiment-nltk-service-multi-instance_0_2 on CPU using artifact 'model.py'\r\n",
      "I0226 05:55:23.633855 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_4RgPui/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_4RgPui/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-nltk-service-multi-instance/1/model.py triton_python_backend_shm_region_6 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-nltk-service-multi-instance_0_2\r\n",
      "I0226 05:55:24.055651 1 python_be.cc:1879] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful sentiment-nltk-service-multi-instance_0_2 (device 0)\r\n",
      "I0226 05:55:24.055793 1 backend_model_instance.cc:766] Starting backend thread for sentiment-nltk-service-multi-instance_0_2 at nice 0 on device 0...\r\n",
      "I0226 05:55:24.055958 1 python_be.cc:1858] TRITONBACKEND_ModelInstanceInitialize: sentiment-nltk-service-multi-instance_0_3 (CPU device 0)\r\n",
      "I0226 05:55:24.055980 1 backend_model_instance.cc:68] Creating instance sentiment-nltk-service-multi-instance_0_3 on CPU using artifact 'model.py'\r\n",
      "I0226 05:55:24.078203 1 stub_launcher.cc:251] Starting Python backend stub: source /tmp/python_env_4RgPui/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_4RgPui/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /mnt/data/model_repository/sentiment-nltk-service-multi-instance/1/model.py triton_python_backend_shm_region_7 67108864 67108864 1 /opt/tritonserver/backends/python 336 sentiment-nltk-service-multi-instance_0_3\r\n",
      "I0226 05:55:24.490593 1 python_be.cc:1879] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful sentiment-nltk-service-multi-instance_0_3 (device 0)\r\n",
      "I0226 05:55:24.490727 1 backend_model_instance.cc:766] Starting backend thread for sentiment-nltk-service-multi-instance_0_3 at nice 0 on device 0...\r\n",
      "I0226 05:55:24.490954 1 model_lifecycle.cc:694] successfully loaded 'sentiment-nltk-service-multi-instance' version 1\r\n",
      "I0226 05:55:24.490960 1 dynamic_batch_scheduler.cc:284] Starting dynamic-batcher thread for sentiment-nltk-service-multi-instance at nice 0...\r\n",
      "I0226 05:55:24.491085 1 server.cc:563] \r\n",
      "+------------------+------+\r\n",
      "| Repository Agent | Path |\r\n",
      "+------------------+------+\r\n",
      "+------------------+------+\r\n",
      "\r\n",
      "I0226 05:55:24.491148 1 server.cc:590] \r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Backend | Path                                                  | Config                                                                                                                                                        |\r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| python  | /opt/tritonserver/backends/python/libtriton_python.so | {\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\r\n",
      "+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0226 05:55:24.491191 1 server.cc:633] \r\n",
      "+---------------------------------------+---------+--------+\r\n",
      "| Model                                 | Version | Status |\r\n",
      "+---------------------------------------+---------+--------+\r\n",
      "| sentiment-nltk-service                | 1       | READY  |\r\n",
      "| sentiment-nltk-service-multi-instance | 1       | READY  |\r\n",
      "+---------------------------------------+---------+--------+\r\n",
      "\r\n",
      "I0226 05:55:24.491215 1 metrics.cc:722] Collecting Response Cache metrics\r\n",
      "I0226 05:55:24.491380 1 metrics.cc:757] Collecting CPU metrics\r\n",
      "I0226 05:55:24.491579 1 tritonserver.cc:2264] \r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| Option                           | Value                                                                                                                                                                                                |\r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "| server_id                        | triton                                                                                                                                                                                               |\r\n",
      "| server_version                   | 2.30.0                                                                                                                                                                                               |\r\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace logging |\r\n",
      "| model_repository_path[0]         | /mnt/data/model_repository                                                                                                                                                                           |\r\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                            |\r\n",
      "| strict_model_config              | 0                                                                                                                                                                                                    |\r\n",
      "| rate_limit                       | OFF                                                                                                                                                                                                  |\r\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                            |\r\n",
      "| response_cache_byte_size         | 1048576                                                                                                                                                                                              |\r\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                                  |\r\n",
      "| strict_readiness                 | 1                                                                                                                                                                                                    |\r\n",
      "| exit_timeout                     | 30                                                                                                                                                                                                   |\r\n",
      "+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n",
      "\r\n",
      "I0226 05:55:24.492218 1 grpc_server.cc:4763] === GRPC KeepAlive Options ===\r\n",
      "I0226 05:55:24.492235 1 grpc_server.cc:4764] keepalive_time_ms: 7200000\r\n",
      "I0226 05:55:24.492240 1 grpc_server.cc:4766] keepalive_timeout_ms: 20000\r\n",
      "I0226 05:55:24.492245 1 grpc_server.cc:4768] keepalive_permit_without_calls: 0\r\n",
      "I0226 05:55:24.492250 1 grpc_server.cc:4770] http2_max_pings_without_data: 2\r\n",
      "I0226 05:55:24.492254 1 grpc_server.cc:4772] http2_min_recv_ping_interval_without_data_ms: 300000\r\n",
      "I0226 05:55:24.492259 1 grpc_server.cc:4775] http2_max_ping_strikes: 2\r\n",
      "I0226 05:55:24.492264 1 grpc_server.cc:4777] ==============================\r\n",
      "I0226 05:55:24.494871 1 grpc_server.cc:225] Ready for RPC 'ServerLive', 0\r\n",
      "I0226 05:55:24.494899 1 grpc_server.cc:225] Ready for RPC 'ServerReady', 0\r\n",
      "I0226 05:55:24.494908 1 grpc_server.cc:225] Ready for RPC 'ModelReady', 0\r\n",
      "I0226 05:55:24.494919 1 grpc_server.cc:225] Ready for RPC 'ServerMetadata', 0\r\n",
      "I0226 05:55:24.494928 1 grpc_server.cc:225] Ready for RPC 'ModelMetadata', 0\r\n",
      "I0226 05:55:24.494958 1 grpc_server.cc:225] Ready for RPC 'ModelConfig', 0\r\n",
      "I0226 05:55:24.494975 1 grpc_server.cc:225] Ready for RPC 'ModelStatistics', 0\r\n",
      "I0226 05:55:24.494987 1 grpc_server.cc:225] Ready for RPC 'Trace', 0\r\n",
      "I0226 05:55:24.494999 1 grpc_server.cc:225] Ready for RPC 'Logging', 0\r\n",
      "I0226 05:55:24.495025 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryStatus', 0\r\n",
      "I0226 05:55:24.495037 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryRegister', 0\r\n",
      "I0226 05:55:24.495044 1 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryUnregister', 0\r\n",
      "I0226 05:55:24.495068 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryStatus', 0\r\n",
      "I0226 05:55:24.495080 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryRegister', 0\r\n",
      "I0226 05:55:24.495088 1 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryUnregister', 0\r\n",
      "I0226 05:55:24.495113 1 grpc_server.cc:225] Ready for RPC 'RepositoryIndex', 0\r\n",
      "I0226 05:55:24.495125 1 grpc_server.cc:225] Ready for RPC 'RepositoryModelLoad', 0\r\n",
      "I0226 05:55:24.495135 1 grpc_server.cc:225] Ready for RPC 'RepositoryModelUnload', 0\r\n",
      "I0226 05:55:24.495174 1 grpc_server.cc:419] Thread started for CommonHandler\r\n",
      "I0226 05:55:24.495331 1 grpc_server.cc:3793] New request handler for ModelInferHandler, 0\r\n",
      "I0226 05:55:24.495380 1 grpc_server.cc:2717] Thread started for ModelInferHandler\r\n",
      "I0226 05:55:24.495501 1 grpc_server.cc:3793] New request handler for ModelInferHandler, 0\r\n",
      "I0226 05:55:24.495547 1 grpc_server.cc:2717] Thread started for ModelInferHandler\r\n",
      "I0226 05:55:24.495651 1 grpc_server.cc:4189] New request handler for ModelStreamInferHandler, 0\r\n",
      "I0226 05:55:24.495688 1 grpc_server.cc:2717] Thread started for ModelStreamInferHandler\r\n",
      "I0226 05:55:24.495700 1 grpc_server.cc:4819] Started GRPCInferenceService at 0.0.0.0:8001\r\n",
      "I0226 05:55:24.495980 1 http_server.cc:3477] Started HTTPService at 0.0.0.0:8000\r\n",
      "I0226 05:55:24.537250 1 http_server.cc:184] Started Metrics Service at 0.0.0.0:8002\r\n"
     ]
    }
   ],
   "source": [
    "!docker logs {container_id[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d148a21",
   "metadata": {},
   "source": [
    "### 1.3 Inference Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c656ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def send_n_concurrent_requests(num_requests, url):\n",
    "    \n",
    "    payload = json.dumps({\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"TEXT\",\n",
    "          \"shape\": [\n",
    "            1,\n",
    "            1\n",
    "          ],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\n",
    "            \"\"\"I think you did a great job when you ran the all-hands meeting.\n",
    "            It showed that you are capable of getting people to work together and communicate effectively.\n",
    "            I admire your communication skills.\n",
    "            One of your most impactful moments was how you handled Project X.\n",
    "            You showed the power of user testing in shaping a feature roadmap.\n",
    "            Your efforts increased the likelihood that we satisfy and delight our users.\n",
    "            I'd love to see you do more of this.\n",
    "            Something I really appreciate about you is your aptitude for problem-solving.\n",
    "            I really think you have a superpower around making new hires feel welcome.\n",
    "            One of the things I admire about you is your ability to manage a team remotely.\n",
    "            I can see you’re having a positive impact in your new office, people seem really happy to have you on their team.\"\"\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    def infer(request_num):\n",
    "        #  print(f\"Processing: {request_num}\")  #. For debugging\n",
    "        response = requests.request(\"POST\",url , headers=headers, data=payload)\n",
    "        return response.elapsed.total_seconds()\n",
    "\n",
    "    response_times = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_requests) as executor:\n",
    "        for result in executor.map(infer, range(num_requests)):\n",
    "            response_times.append(result)\n",
    "\n",
    "    print(f\"Total Time taken: {sum(response_times)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0cbfad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time taken: 154.6507970000001\n"
     ]
    }
   ],
   "source": [
    "send_n_concurrent_requests(1000,\"http://localhost:8000/v2/models/sentiment-nltk-service/versions/1/infer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4324e940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time taken: 13.099421999999999\n"
     ]
    }
   ],
   "source": [
    "send_n_concurrent_requests(1000,\"http://localhost:8000/v2/models/sentiment-nltk-service-multi-instance/versions/1/infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c7960",
   "metadata": {},
   "source": [
    "### 2.4 Cleanups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6138a284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07be1398dba9332f55919abb0d9f1d75d1c5fc1d4c13fdcd0aca8781ff28ac5a\r\n"
     ]
    }
   ],
   "source": [
    "!docker stop {container_id[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a5455db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! This will remove all stopped containers.\r\n",
      "Are you sure you want to continue? [y/N] Deleted Containers:\r\n",
      "07be1398dba9332f55919abb0d9f1d75d1c5fc1d4c13fdcd0aca8781ff28ac5a\r\n",
      "\r\n",
      "Total reclaimed space: 12.55kB\r\n"
     ]
    }
   ],
   "source": [
    "!echo y | docker container prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dde010",
   "metadata": {},
   "source": [
    "# IV. Optimisation Features - Introductions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49bcc6",
   "metadata": {},
   "source": [
    "## 1. Dynamic Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3dd80689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"sentiment-nltk-service\"\r\n",
      "backend: \"python\"  # PyTorch, TF, ONNX, TensorRT\r\n",
      "max_batch_size: 8\r\n",
      "\r\n",
      "dynamic_batching { }\r\n",
      "\r\n",
      "input [\r\n",
      "  {\r\n",
      "    name: \"TEXT\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims:  [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "output [\r\n",
      "  {\r\n",
      "    name: \"STATUS\"\r\n",
      "    data_type: TYPE_STRING\r\n",
      "    dims: [1]\r\n",
      "  },\r\n",
      "  {\r\n",
      "    name: \"SCORE\"\r\n",
      "    data_type: TYPE_FP32\r\n",
      "    dims: [1]\r\n",
      "  }\r\n",
      "]\r\n",
      "\r\n",
      "parameters: {\r\n",
      "  key: \"EXECUTION_ENV_PATH\",\r\n",
      "  value: {string_value: \"/mnt/data/model_repository/sentiment-nltk-service/sentinltkenv.tar.gz\"}\r\n",
      "}\r\n",
      "\r\n",
      "instance_group [\r\n",
      "  {\r\n",
      "    count: 1\r\n",
      "    kind: KIND_CPU\r\n",
      "  }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!cat model_repository_single_service/sentiment-nltk-service/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73b6e4",
   "metadata": {},
   "source": [
    "## 2. Ensembler and BLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d97f3",
   "metadata": {},
   "source": [
    "<img src=\"Ensemble.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0320f",
   "metadata": {},
   "source": [
    "## 3. Model Analyser"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c4c6ab7",
   "metadata": {},
   "source": [
    "# Config.yaml\n",
    "\n",
    "model_repository: /home/model_repo\n",
    "\n",
    "run_config_search_disable: True\n",
    "\n",
    "concurrency: [2, 4, 8, 16, 32]\n",
    "batch_sizes: [8, 16, 64]\n",
    "\n",
    "profile_models:\n",
    "  classification_malaria_v1:\n",
    "    model_config_parameters:\n",
    "      instance_group:\n",
    "        - kind: KIND_GPU\n",
    "          count: [1, 2]\n",
    "      dynamic_batching:\n",
    "        max_queue_delay_microseconds: [100]\n",
    "  classification_chestxray_v1:\n",
    "    model_config_parameters:\n",
    "      instance_group:\n",
    "        - kind: KIND_GPU\n",
    "          count: [1, 2]\n",
    "      dynamic_batching:\n",
    "        max_queue_delay_microseconds: [100]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e623093",
   "metadata": {},
   "source": [
    "model-analyzer profile -f /path/to/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ede26",
   "metadata": {},
   "source": [
    "# That's all folks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87606680",
   "metadata": {},
   "source": [
    "<i> <h3> Meet you in Advanced Triton Optimisation Features session! </h3> </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4411a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
